{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fdec1",
   "metadata": {},
   "source": [
    "# MyUNet.py として分割予定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d271db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.enc1 = DoubleConv(n_channels, 64)\n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec4 = DoubleConv(128, 64)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
    "        e4 = self.enc4(F.max_pool2d(e3, 2))\n",
    "        b = self.bottleneck(F.max_pool2d(e4, 2))\n",
    "        d1 = self.up1(b)\n",
    "        # d1のサイズに合わせてe4をクロップ\n",
    "        e4_cropped = self.crop_tensor(e4, d1)\n",
    "        d1_cat = torch.cat([e4_cropped, d1], dim=1)\n",
    "        d1 = self.dec1(d1_cat)\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        # d2のサイズに合わせてe3をクロップ\n",
    "        e3_cropped = self.crop_tensor(e3, d2)\n",
    "        d2_cat = torch.cat([e3_cropped, d2], dim=1)\n",
    "        d2 = self.dec2(d2_cat)\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        # d3のサイズに合わせてe2をクロップ\n",
    "        e2_cropped = self.crop_tensor(e2, d3)\n",
    "        d3_cat = torch.cat([e2_cropped, d3], dim=1)\n",
    "        d3 = self.dec3(d3_cat)\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        # d4のサイズに合わせてe1をクロップ\n",
    "        e1_cropped = self.crop_tensor(e1, d4)\n",
    "        d4_cat = torch.cat([e1_cropped, d4], dim=1)\n",
    "        d4 = self.dec4(d4_cat)\n",
    "\n",
    "        return self.out_conv(d4)\n",
    "\n",
    "    def crop_tensor(self, source, target):\n",
    "        \"\"\"\n",
    "        sourceテンソルをtargetテンソルのサイズに合わせて中央でクロップする\n",
    "        \"\"\"\n",
    "        target_size_h = target.size()[2]\n",
    "        target_size_w = target.size()[3]\n",
    "        source_size_h = source.size()[2]\n",
    "        source_size_w = source.size()[3]\n",
    "        \n",
    "        # クロップの開始位置を計算 (中央揃え)\n",
    "        delta_h = (source_size_h - target_size_h) // 2\n",
    "        delta_w = (source_size_w - target_size_w) // 2\n",
    "        \n",
    "        # スライシングでクロップ\n",
    "        return source[:, :, delta_h:delta_h + target_size_h, delta_w:delta_w + target_size_w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249db223",
   "metadata": {},
   "source": [
    "# Mydataset.pyとして分割予定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms.functional as functinal\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 test_doc_id_list,\n",
    "                 test_mode = False,\n",
    "                #  input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                 input_path = '../kuzushiji_recognition/synthetic_images/tmp_entire_data/',\n",
    "                 json_path = '../kuzushiji_recognition/synthetic_images/gt_json.json',\n",
    "                 transform = None,\n",
    "                 image_downsample_rate = 10):\n",
    "        super().__init__()\n",
    "        self.test_doc_id_list = test_doc_id_list\n",
    "        self.input_path = input_path\n",
    "        self.transform = transform\n",
    "        self.image_downsample_rate = image_downsample_rate\n",
    "        # 画像のIDをリストにして保管。\n",
    "        self.input_imageID_list = []\n",
    "        for file_name in os.listdir(self.input_path):\n",
    "            file_path = os.path.join(self.input_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                if not (file_name.split('_sep_')[0] in self.test_doc_id_list) ^ test_mode:\n",
    "                    self.input_imageID_list.append(file_name.split('.')[0])\n",
    "        # アノテーションデータを保持するjsonファイルをロード\n",
    "        self.gt_json = self.load_GT_json(json_path)\n",
    "\n",
    "        # 入力画像に対応するアノテーションデータが存在するか確認。修正\n",
    "        # for i in range(len(self.input_imageID_list)-1, -1,-1):\n",
    "        #     if not self.input_imageID_list[i] in self.gt_json:\n",
    "        #         del self.input_imageID_list[i]\n",
    "    def __len__(self):\n",
    "        return len(self.input_imageID_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.input_imageID_list[index]\n",
    "        image = Image.open(self.input_path+image_id+'.jpg')\n",
    "        # ここで正解マップを復元 ( 4チャネルの torch tensor を出力。)\n",
    "        tensor_gt = self.return_tensor_gt(gt_info_dic=self.gt_json['files'][image_id], image=image)\n",
    "        \n",
    "        # 1. 元の画像のサイズを取得\n",
    "        original_w, original_h = image.size\n",
    "        # 2. ターゲットとなる新しいサイズを計算 (高さ、幅を半分に)\n",
    "        new_size = (original_h //self.image_downsample_rate , original_w // self.image_downsample_rate)\n",
    "        # 3. functional.resizeを使って、入力画像と正解データの両方をリサイズ\n",
    "        #    補間方法(interpolation)は、連続値なので両方ともBILINEAR(双線形補間)が適しています。\n",
    "        image = functinal.resize(image, new_size, interpolation=functinal.InterpolationMode.BILINEAR)\n",
    "        tensor_gt = functinal.resize(tensor_gt, new_size, interpolation=functinal.InterpolationMode.BILINEAR)\n",
    "\n",
    "        image = self.transform(image)\n",
    "        return image, tensor_gt\n",
    "\n",
    "    def load_GT_json(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # print('check : ', end='')\n",
    "            # print(f.readlines()[-1])\n",
    "            data = json.load(f)\n",
    "        print(\"json データを読み込みました。\")\n",
    "        return data\n",
    "\n",
    "    def return_ground_truth_canvas(self, image):\n",
    "        w, h = image.size\n",
    "        main_region = np.zeros((h, w), dtype=np.float64)\n",
    "        main_affinity = np.zeros((h, w), dtype=np.float64)\n",
    "        furi_region = np.zeros((h, w), dtype=np.float64)\n",
    "        furi_affinity = np.zeros((h, w), dtype=np.float64)\n",
    "        return main_region, main_affinity, furi_region, furi_affinity\n",
    "\n",
    "    def add_perspective_gaussian_to_canvas(self, canvas, points, amplitude=1.0):\n",
    "        # 領域の4点を取得\n",
    "        src_points = np.array(points, dtype=np.float32)\n",
    "\n",
    "        # ガウス分布を生成するための仮想的な正方形領域を定義\n",
    "        width = int(max(np.linalg.norm(src_points[0] - src_points[1]), np.linalg.norm(src_points[2] - src_points[3])))\n",
    "        height = int(max(np.linalg.norm(src_points[0] - src_points[3]), np.linalg.norm(src_points[1] - src_points[2])))\n",
    "        dst_points = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=np.float32)\n",
    "\n",
    "        # ガウス分布を生成\n",
    "        x = np.linspace(-width / 2, width / 2, width)\n",
    "        y = np.linspace(-height / 2, height / 2, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        sigma_x = width / 5.0\n",
    "        sigma_y = height / 5.0\n",
    "        gaussian = amplitude * np.exp(-((x**2) / (2 * sigma_x**2) + (y**2) / (2 * sigma_y**2)))\n",
    "\n",
    "        # Perspective Transformation行列を計算\n",
    "        matrix = cv2.getPerspectiveTransform(dst_points, src_points)\n",
    "\n",
    "        # ガウス分布をPerspective Transformationで変形\n",
    "        transformed_gaussian = cv2.warpPerspective(gaussian, matrix, (canvas.shape[1], canvas.shape[0]))\n",
    "\n",
    "        # キャンバスにガウス分布を追加\n",
    "        canvas += transformed_gaussian\n",
    "\n",
    "        return canvas\n",
    "    # <main_region と、gt_info_dic['main_region'] を入力に main_region をデザインし、ガウス分布マップを出力する。>\n",
    "    def design_gaussian_map(self, canvas, point_list):    \n",
    "        for points in point_list:\n",
    "            p1x, p1y, p2x, p2y, p3x, p3y, p4x, p4y = points\n",
    "            canvas = PreTrainDataset.add_perspective_gaussian_to_canvas('_', canvas, ((p1x, p1y), (p2x, p2y), (p3x, p3y), (p4x, p4y)), amplitude=1.0)\n",
    "        return canvas\n",
    "    # < main_region などの辞書と image を入力にとり、4チャネルの torch tensor を出力する。>\n",
    "    def return_tensor_gt(self, gt_info_dic, image):\n",
    "        main_region, main_affinity, furi_region, furi_affinity = PreTrainDataset.return_ground_truth_canvas('_', image)\n",
    "\n",
    "        # <main_region, main_affinity, furi_region, furi_affinity をgt_info に基づいてデザイン>\n",
    "        canvas_list = []\n",
    "        for canvas in ['main_region', 'main_affinity', 'furi_region', 'furi_affinity']:\n",
    "            canvas_list.append(self.design_gaussian_map(eval(canvas), gt_info_dic[canvas]))\n",
    "\n",
    "        # <それぞれをtorch tensor に変換して、4チャネルのtorch tensor>\n",
    "        tensor_list = []\n",
    "        for canvas in canvas_list:\n",
    "            tensor_list.append(torch.from_numpy(canvas.astype(np.float32)).clone())\n",
    "        return_tensor = torch.stack((tensor_list[0],tensor_list[1],tensor_list[2],tensor_list[3]))\n",
    "        return return_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089a6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms.functional as functional\n",
    "from typing import List, Tuple\n",
    "import multiprocessing as mp\n",
    "from functools import lru_cache\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 test_doc_id_list,\n",
    "                 test_mode = False,\n",
    "                 input_path = '../kuzushiji-recognition/synthetic_images/input_images/',\n",
    "                 json_path = '../kuzushiji-recognition/synthetic_images/gt_json.json',\n",
    "                 transform = None,\n",
    "                 image_downsample_rate = 10,\n",
    "                 device = None,\n",
    "                 precompute_gt = True,\n",
    "                 num_workers = None):\n",
    "        super().__init__()\n",
    "        self.test_doc_id_list = test_doc_id_list\n",
    "        self.input_path = input_path\n",
    "        self.transform = transform\n",
    "        self.image_downsample_rate = image_downsample_rate\n",
    "        \n",
    "        # デバイス設定\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # ワーカー数設定\n",
    "        if num_workers is None:\n",
    "            self.num_workers = min(mp.cpu_count(), 4)\n",
    "        else:\n",
    "            self.num_workers = num_workers\n",
    "        \n",
    "        # 画像のIDをリストにして保管\n",
    "        self.input_imageID_list = []\n",
    "        for file_name in os.listdir(self.input_path):\n",
    "            file_path = os.path.join(self.input_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                if not (file_name.split('_sep_')[0] in self.test_doc_id_list) ^ test_mode:\n",
    "                    self.input_imageID_list.append(file_name.split('.')[0])\n",
    "        \n",
    "        # アノテーションデータを保持するjsonファイルをロード\n",
    "        self.gt_json = self.load_GT_json(json_path)\n",
    "        \n",
    "        # 正解データの事前計算（オプション）\n",
    "        self.precomputed_gt = {}\n",
    "        if precompute_gt:\n",
    "            print(\"Pre-computing ground truth data...\")\n",
    "            self._precompute_ground_truth()\n",
    "            print(\"Pre-computation completed.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_imageID_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.input_imageID_list[index]\n",
    "        image = Image.open(self.input_path + image_id + '.jpg')\n",
    "        \n",
    "        # 事前計算されたデータがあれば使用\n",
    "        if image_id in self.precomputed_gt:\n",
    "            tensor_gt = self.precomputed_gt[image_id]\n",
    "        else:\n",
    "            # リアルタイムで正解データを生成\n",
    "            tensor_gt = self.return_tensor_gt_optimized(\n",
    "                gt_info_dic=self.gt_json['files'][image_id], \n",
    "                image=image\n",
    "            )\n",
    "        \n",
    "        # 1. 元の画像のサイズを取得\n",
    "        original_w, original_h = image.size\n",
    "        # 2. ターゲットとなる新しいサイズを計算\n",
    "        new_size = (original_h // self.image_downsample_rate, original_w // self.image_downsample_rate)\n",
    "        \n",
    "        # 3. リサイズ処理\n",
    "        image = functional.resize(image, new_size, interpolation=functional.InterpolationMode.BILINEAR)\n",
    "        tensor_gt = F.interpolate(\n",
    "            tensor_gt.unsqueeze(0), \n",
    "            size=new_size, \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, tensor_gt\n",
    "\n",
    "    def _precompute_ground_truth(self):\n",
    "        \"\"\"正解データを事前計算してメモリに保存\"\"\"\n",
    "        for image_id in self.input_imageID_list:\n",
    "            try:\n",
    "                image = Image.open(self.input_path + image_id + '.jpg')\n",
    "                tensor_gt = self.return_tensor_gt_optimized(\n",
    "                    gt_info_dic=self.gt_json['files'][image_id], \n",
    "                    image=image\n",
    "                )\n",
    "                self.precomputed_gt[image_id] = tensor_gt\n",
    "            except Exception as e:\n",
    "                print(f\"Error precomputing {image_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "    def load_GT_json(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"jsonデータを読み込みました。\")\n",
    "        return data\n",
    "\n",
    "    def return_tensor_gt_optimized(self, gt_info_dic, image):\n",
    "        \"\"\"最適化された正解データ生成メソッド\"\"\"\n",
    "        w, h = image.size\n",
    "        \n",
    "        # GPUでテンソルを直接作成\n",
    "        canvas_tensors = torch.zeros(4, h, w, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # 各チャネルを並列処理\n",
    "        channel_names = ['main_region', 'main_affinity', 'furi_region', 'furi_affinity']\n",
    "        \n",
    "        for i, channel_name in enumerate(channel_names):\n",
    "            if channel_name in gt_info_dic and gt_info_dic[channel_name]:\n",
    "                canvas_tensors[i] = self.design_gaussian_map_gpu(\n",
    "                    canvas_tensors[i], \n",
    "                    gt_info_dic[channel_name], \n",
    "                    w, h\n",
    "                )\n",
    "        \n",
    "        return canvas_tensors\n",
    "\n",
    "    def design_gaussian_map_gpu(self, canvas_tensor, point_list, width, height):\n",
    "        \"\"\"GPU上でガウス分布マップを生成\"\"\"\n",
    "        if not point_list:\n",
    "            return canvas_tensor\n",
    "            \n",
    "        # バッチ処理のためにポイントリストを整理\n",
    "        batch_points = torch.tensor(point_list, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        for points in batch_points:\n",
    "            p1x, p1y, p2x, p2y, p3x, p3y, p4x, p4y = points\n",
    "            \n",
    "            # 四角形の各頂点\n",
    "            src_points = torch.tensor([\n",
    "                [p1x, p1y], [p2x, p2y], [p3x, p3y], [p4x, p4y]\n",
    "            ], dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            canvas_tensor = self.add_perspective_gaussian_gpu(\n",
    "                canvas_tensor, src_points, width, height\n",
    "            )\n",
    "        \n",
    "        return canvas_tensor\n",
    "\n",
    "    def add_perspective_gaussian_gpu(self, canvas, src_points, canvas_width, canvas_height):\n",
    "        \"\"\"GPU上で透視変換されたガウス分布を追加\"\"\"\n",
    "        # 四角形のサイズを計算\n",
    "        width = max(\n",
    "            torch.norm(src_points[0] - src_points[1]).item(),\n",
    "            torch.norm(src_points[2] - src_points[3]).item()\n",
    "        )\n",
    "        height = max(\n",
    "            torch.norm(src_points[0] - src_points[3]).item(),\n",
    "            torch.norm(src_points[1] - src_points[2]).item()\n",
    "        )\n",
    "        \n",
    "        width = int(width) + 1\n",
    "        height = int(height) + 1\n",
    "        \n",
    "        # ガウス分布を生成\n",
    "        gaussian = self.create_gaussian_kernel_gpu(width, height)\n",
    "        \n",
    "        # 透視変換行列を計算（CPUで実行）\n",
    "        src_np = src_points.cpu().numpy()\n",
    "        dst_np = np.array([\n",
    "            [0, 0], [width-1, 0], [width-1, height-1], [0, height-1]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        try:\n",
    "            matrix = cv2.getPerspectiveTransform(dst_np, src_np)\n",
    "            \n",
    "            # 変換をGPU上で実行\n",
    "            transformed_gaussian = self.warp_perspective_gpu(\n",
    "                gaussian, matrix, canvas_width, canvas_height\n",
    "            )\n",
    "            \n",
    "            canvas += transformed_gaussian\n",
    "            \n",
    "        except cv2.error:\n",
    "            # 透視変換が失敗した場合はスキップ\n",
    "            pass\n",
    "            \n",
    "        return canvas\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    def create_gaussian_kernel_gpu(self, width, height):\n",
    "        \"\"\"GPU上でガウシアンカーネルを生成（キャッシュ付き）\"\"\"\n",
    "        x = torch.linspace(-width/2, width/2, width, device=self.device)\n",
    "        y = torch.linspace(-height/2, height/2, height, device=self.device)\n",
    "        \n",
    "        # メッシュグリッドを作成\n",
    "        y_grid, x_grid = torch.meshgrid(y, x, indexing='ij')\n",
    "        \n",
    "        # ガウス分布のパラメータ\n",
    "        sigma_x = width / 5.0\n",
    "        sigma_y = height / 5.0\n",
    "        \n",
    "        # ガウス分布を計算\n",
    "        gaussian = torch.exp(-(x_grid**2 / (2 * sigma_x**2) + y_grid**2 / (2 * sigma_y**2)))\n",
    "        \n",
    "        return gaussian\n",
    "\n",
    "    def warp_perspective_gpu(self, image_tensor, matrix, output_width, output_height):\n",
    "        \"\"\"GPU上で透視変換を実行\"\"\"\n",
    "        # 変換行列をテンソルに変換\n",
    "        matrix_tensor = torch.from_numpy(matrix).float().to(self.device)\n",
    "        \n",
    "        # グリッドを生成\n",
    "        grid = self.create_transformation_grid(\n",
    "            matrix_tensor, output_height, output_width\n",
    "        )\n",
    "        \n",
    "        # grid_sampleを使用して変換\n",
    "        image_batch = image_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
    "        grid_batch = grid.unsqueeze(0)  # [1, H, W, 2]\n",
    "        \n",
    "        transformed = F.grid_sample(\n",
    "            image_batch, grid_batch, \n",
    "            mode='bilinear', \n",
    "            padding_mode='zeros',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return transformed.squeeze(0).squeeze(0)\n",
    "\n",
    "    def create_transformation_grid(self, matrix, height, width):\n",
    "        \"\"\"変換グリッドを作成\"\"\"\n",
    "        # 出力座標を生成\n",
    "        y_coords = torch.arange(height, dtype=torch.float32, device=self.device)\n",
    "        x_coords = torch.arange(width, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "        \n",
    "        # 同次座標に変換\n",
    "        ones = torch.ones_like(x_grid)\n",
    "        coords = torch.stack([x_grid, y_grid, ones], dim=-1)  # [H, W, 3]\n",
    "        \n",
    "        # 逆変換行列を適用\n",
    "        try:\n",
    "            inv_matrix = torch.inverse(matrix)\n",
    "        except:\n",
    "            # 逆行列が計算できない場合は単位行列を使用\n",
    "            inv_matrix = torch.eye(3, device=self.device)\n",
    "        \n",
    "        # 変換を適用\n",
    "        transformed_coords = torch.matmul(coords, inv_matrix.T)  # [H, W, 3]\n",
    "        \n",
    "        # 正規化座標に変換\n",
    "        x_norm = transformed_coords[..., 0] / transformed_coords[..., 2]\n",
    "        y_norm = transformed_coords[..., 1] / transformed_coords[..., 2]\n",
    "        \n",
    "        # grid_sampleの座標系に変換 [-1, 1]\n",
    "        grid_x = 2.0 * x_norm / (width - 1) - 1.0\n",
    "        grid_y = 2.0 * y_norm / (height - 1) - 1.0\n",
    "        \n",
    "        grid = torch.stack([grid_x, grid_y], dim=-1)\n",
    "        \n",
    "        return grid\n",
    "\n",
    "\n",
    "# 使用例とパフォーマンス比較\n",
    "def benchmark_dataset(dataset, num_samples=10):\n",
    "    \"\"\"データセットのパフォーマンスをベンチマーク\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        image, gt = dataset[i]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_samples\n",
    "    print(f\"Average time per sample: {avg_time:.4f} seconds\")\n",
    "    return avg_time\n",
    "\n",
    "\n",
    "# データローダー用の高速化設定\n",
    "def create_optimized_dataloader(dataset, batch_size=8, num_workers=4):\n",
    "    \"\"\"最適化されたDataLoaderを作成\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=2 if num_workers > 0 else 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c611ba8",
   "metadata": {},
   "source": [
    "# メインの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     13\u001b[0m test_doc_id_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100241706\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249371\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249376\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249416\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249476\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249537\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003076\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003803\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003967\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200004107\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# train_dataset = PreTrainDataset(test_doc_id_list,\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#                             test_mode = False,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#                             transform = transform)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 高速化されたデータセットの作成\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(\n\u001b[0;32m     28\u001b[0m     test_doc_id_list\u001b[38;5;241m=\u001b[39mtest_doc_id_list,\n\u001b[0;32m     29\u001b[0m     test_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m     device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m),  \u001b[38;5;66;03m# GPUを明示的に指定\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     precompute_gt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 事前計算を有効化\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# num_workers=None\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(\n\u001b[0;32m     35\u001b[0m     test_doc_id_list\u001b[38;5;241m=\u001b[39mtest_doc_id_list,\n\u001b[0;32m     36\u001b[0m     test_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# num_workers=4\u001b[39;00m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 最適化されたDataLoaderの作成\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mPreTrainDataset.__init__\u001b[1;34m(self, test_doc_id_list, test_mode, input_path, json_path, transform, image_downsample_rate, device, precompute_gt, num_workers)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_imageID_list\u001b[38;5;241m.\u001b[39mappend(file_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# アノテーションデータを保持するjsonファイルをロード\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_GT_json(json_path)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 正解データの事前計算（オプション）\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed_gt \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[4], line 115\u001b[0m, in \u001b[0;36mPreTrainDataset.load_GT_json\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_GT_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 115\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjsonデータを読み込みました。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\kotat\\AppData\\Local\\anaconda3\\envs\\kuzushiji\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\kotat\\AppData\\Local\\anaconda3\\envs\\kuzushiji\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\kotat\\AppData\\Local\\anaconda3\\envs\\kuzushiji\\Lib\\json\\decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m--> 338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = False,\n",
    "                            # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "                            json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "                            transform = transform)\n",
    "test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = True,\n",
    "                            # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "                            json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "                            transform = transform)\n",
    "\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "test_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "# --- データセットとデータローダの準備 ---\n",
    "# train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 学習ループの拡張 ---\n",
    "\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "学習を開始します...\n",
      "start epcoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]:   0%|          | 0/4965 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "# train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "#                             test_mode = False,\n",
    "#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "#                             input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "#                             json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "#                             transform = transform)\n",
    "# test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "#                             test_mode = True,\n",
    "#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "#                             input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "#                             json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "#                             transform = transform)\n",
    "\n",
    "train_dataset = PreTrainDataset(\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=False,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=False,  # 事前計算を有効化\n",
    "    # num_workers=None\n",
    ")\n",
    "test_dataset = PreTrainDataset(\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=True,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=False,  # 事前計算を有効化\n",
    "    # num_workers=4\n",
    ")\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "test_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "# --- データセットとデータローダの準備 ---\n",
    "# train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 学習ループの拡張 ---\n",
    "\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a942a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3e29d",
   "metadata": {},
   "source": [
    "# PreTrainDataset　がうまく動作するか確認のためのメイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_doc_id_list = ['100241706']\n",
    "train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = False,\n",
    "                            input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            json_path = '../kuzushiji_recognition/synthetic_images/gt_json.json',\n",
    "                            transform = None)\n",
    "test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = True,\n",
    "                            input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            json_path = '../kuzushiji_recognition/synthetic_images/gt_json.json',\n",
    "                            transform = None)\n",
    "train_dl = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for imgs, masks in train_dl:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a192ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107', '200004148', '200005598', '200005798', '200006663', '200006665', '200008003', '200008316', '200010454', '200014685', '200014740', '200015779', '200015843', '200017458', '200018243', '200019865', '200020019', '200021063', '200021071', '200021086', '200021637', '200021644', '200021660', '200021712', '200021763', '200021802', '200021851', '200021853', '200021869', '200021925', '200022050', '200025191']\n"
     ]
    }
   ],
   "source": [
    "aa = '100241706, 100249371, 100249376, 100249416, 100249476, 100249537, 200003076, 200003803, 200003967, 200004107, 200004148, 200005598, 200005798, 200006663, 200006665, 200008003, 200008316, 200010454, 200014685, 200014740, 200015779, 200015843, 200017458, 200018243, 200019865, 200020019, 200021063, 200021071, 200021086, 200021637, 200021644, 200021660, 200021712, 200021763, 200021802, 200021851, 200021853, 200021869, 200021925, 200022050, 200025191'\n",
    "print(aa.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30544e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuzushiji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
