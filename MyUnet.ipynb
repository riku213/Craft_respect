{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fdec1",
   "metadata": {},
   "source": [
    "# MyUNet.py として分割予定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271db2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249db223",
   "metadata": {},
   "source": [
    "# Mydataset.pyとして分割予定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a6a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c611ba8",
   "metadata": {},
   "source": [
    "# メインの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "Pre-computing ground truth data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# --- Dataset ---\u001b[39;00m\n\u001b[0;32m     24\u001b[0m test_doc_id_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100241706\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249371\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249376\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249416\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249476\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249537\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003076\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003803\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003967\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200004107\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 25\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(test_doc_id_list,\n\u001b[0;32m     26\u001b[0m                             test_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m                             \u001b[38;5;66;03m# input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\u001b[39;00m\n\u001b[0;32m     28\u001b[0m                             input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/input_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m                             json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m                             transform \u001b[38;5;241m=\u001b[39m transform)\n\u001b[0;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(test_doc_id_list,\n\u001b[0;32m     32\u001b[0m                             test_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m                             \u001b[38;5;66;03m# input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\u001b[39;00m\n\u001b[0;32m     34\u001b[0m                             input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/input_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m                             json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     36\u001b[0m                             transform \u001b[38;5;241m=\u001b[39m transform)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 最適化されたDataLoaderの作成\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mPreTrainDataset.__init__\u001b[1;34m(self, test_doc_id_list, test_mode, input_path, json_path, transform, image_downsample_rate, device, precompute_gt, num_workers)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precompute_gt:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-computing ground truth data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precompute_ground_truth()\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-computation completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m, in \u001b[0;36mPreTrainDataset._precompute_ground_truth\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_path \u001b[38;5;241m+\u001b[39m image_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m     tensor_gt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_tensor_gt_optimized(\n\u001b[0;32m    105\u001b[0m         gt_info_dic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m'\u001b[39m][image_id], \n\u001b[0;32m    106\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed_gt[image_id] \u001b[38;5;241m=\u001b[39m tensor_gt\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[3], line 131\u001b[0m, in \u001b[0;36mPreTrainDataset.return_tensor_gt_optimized\u001b[1;34m(self, gt_info_dic, image)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, channel_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(channel_names):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m channel_name \u001b[38;5;129;01min\u001b[39;00m gt_info_dic \u001b[38;5;129;01mand\u001b[39;00m gt_info_dic[channel_name]:\n\u001b[1;32m--> 131\u001b[0m         canvas_tensors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_gaussian_map_gpu(\n\u001b[0;32m    132\u001b[0m             canvas_tensors[i], \n\u001b[0;32m    133\u001b[0m             gt_info_dic[channel_name], \n\u001b[0;32m    134\u001b[0m             w, h\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canvas_tensors\n",
      "Cell \u001b[1;32mIn[3], line 155\u001b[0m, in \u001b[0;36mPreTrainDataset.design_gaussian_map_gpu\u001b[1;34m(self, canvas_tensor, point_list, width, height)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# 四角形の各頂点\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     src_points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[0;32m    152\u001b[0m         [p1x, p1y], [p2x, p2y], [p3x, p3y], [p4x, p4y]\n\u001b[0;32m    153\u001b[0m     ], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 155\u001b[0m     canvas_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_perspective_gaussian_gpu(\n\u001b[0;32m    156\u001b[0m         canvas_tensor, src_points, width, height\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canvas_tensor\n",
      "Cell \u001b[1;32mIn[3], line 189\u001b[0m, in \u001b[0;36mPreTrainDataset.add_perspective_gaussian_gpu\u001b[1;34m(self, canvas, src_points, canvas_width, canvas_height)\u001b[0m\n\u001b[0;32m    186\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetPerspectiveTransform(dst_np, src_np)\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# 変換をGPU上で実行\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     transformed_gaussian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarp_perspective_gpu(\n\u001b[0;32m    190\u001b[0m         gaussian, matrix, canvas_width, canvas_height\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m     canvas \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformed_gaussian\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 透視変換が失敗した場合はスキップ\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 222\u001b[0m, in \u001b[0;36mPreTrainDataset.warp_perspective_gpu\u001b[1;34m(self, image_tensor, matrix, output_width, output_height)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"GPU上で透視変換を実行\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# 変換行列をテンソルに変換\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m matrix_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(matrix)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# グリッドを生成\u001b[39;00m\n\u001b[0;32m    225\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_transformation_grid(\n\u001b[0;32m    226\u001b[0m     matrix_tensor, output_height, output_width\n\u001b[0;32m    227\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = False,\n",
    "                            # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "                            json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "                            transform = transform)\n",
    "test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = True,\n",
    "                            # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "                            json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "                            transform = transform)\n",
    "\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "test_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "# --- データセットとデータローダの準備 ---\n",
    "# train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 学習ループの拡張 ---\n",
    "\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "学習を開始します...\n",
      "start epcoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]:   0%|          | 0/4965 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "# train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "#                             test_mode = False,\n",
    "#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "#                             input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "#                             json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "#                             transform = transform)\n",
    "# test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "#                             test_mode = True,\n",
    "#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "#                             input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "#                             json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "#                             transform = transform)\n",
    "\n",
    "train_dataset = PreTrainDataset(\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=False,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=False,  # 事前計算を有効化\n",
    "    # num_workers=None\n",
    ")\n",
    "test_dataset = PreTrainDataset(\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=True,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=False,  # 事前計算を有効化\n",
    "    # num_workers=4\n",
    ")\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "test_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "# --- データセットとデータローダの準備 ---\n",
    "# train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 学習ループの拡張 ---\n",
    "\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuzushiji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
