{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fdec1",
   "metadata": {},
   "source": [
    "# MyUNet.py として分割予定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271db2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249db223",
   "metadata": {},
   "source": [
    "# Mydataset.pyとして分割予定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a6a66",
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms.functional as functional\n",
    "from typing import List, Tuple\n",
    "import multiprocessing as mp\n",
    "from functools import lru_cache\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 test_doc_id_list,\n",
    "                 test_mode = False,\n",
    "                 input_path = '../kuzushiji-recognition/synthetic_images/input_images/',\n",
    "                 json_path = '../kuzushiji-recognition/synthetic_images/gt_json.json',\n",
    "                 transform = None,\n",
    "                 image_downsample_rate = 10,\n",
    "                 device = None,\n",
    "                 precompute_gt = True,\n",
    "                 num_workers = None):\n",
    "        super().__init__()\n",
    "        self.test_doc_id_list = test_doc_id_list\n",
    "        self.input_path = input_path\n",
    "        self.transform = transform\n",
    "        self.image_downsample_rate = image_downsample_rate\n",
    "        \n",
    "        # デバイス設定\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # ワーカー数設定\n",
    "        if num_workers is None:\n",
    "            self.num_workers = min(mp.cpu_count(), 4)\n",
    "        else:\n",
    "            self.num_workers = num_workers\n",
    "        \n",
    "        # 画像のIDをリストにして保管\n",
    "        self.input_imageID_list = []\n",
    "        for file_name in os.listdir(self.input_path):\n",
    "            file_path = os.path.join(self.input_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                if not (file_name.split('_sep_')[0] in self.test_doc_id_list) ^ test_mode:\n",
    "                    self.input_imageID_list.append(file_name.split('.')[0])\n",
    "        \n",
    "        # アノテーションデータを保持するjsonファイルをロード\n",
    "        self.gt_json = self.load_GT_json(json_path)\n",
    "        \n",
    "        # 正解データの事前計算（オプション）\n",
    "        self.precomputed_gt = {}\n",
    "        if precompute_gt:\n",
    "            print(\"Pre-computing ground truth data...\")\n",
    "            self._precompute_ground_truth()\n",
    "            print(\"Pre-computation completed.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_imageID_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.input_imageID_list[index]\n",
    "        image = Image.open(self.input_path + image_id + '.jpg')\n",
    "        \n",
    "        # 事前計算されたデータがあれば使用\n",
    "        if image_id in self.precomputed_gt:\n",
    "            tensor_gt = self.precomputed_gt[image_id]\n",
    "        else:\n",
    "            # リアルタイムで正解データを生成\n",
    "            tensor_gt = self.return_tensor_gt_optimized(\n",
    "                gt_info_dic=self.gt_json['files'][image_id], \n",
    "                image=image\n",
    "            )\n",
    "        \n",
    "        # 1. 元の画像のサイズを取得\n",
    "        original_w, original_h = image.size\n",
    "        # 2. ターゲットとなる新しいサイズを計算\n",
    "        new_size = (original_h // self.image_downsample_rate, original_w // self.image_downsample_rate)\n",
    "        \n",
    "        # 3. リサイズ処理\n",
    "        image = functional.resize(image, new_size, interpolation=functional.InterpolationMode.BILINEAR)\n",
    "        tensor_gt = F.interpolate(\n",
    "            tensor_gt.unsqueeze(0), \n",
    "            size=new_size, \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, tensor_gt\n",
    "\n",
    "    def _precompute_ground_truth(self):\n",
    "        \"\"\"正解データを事前計算してメモリに保存\"\"\"\n",
    "        for image_id in self.input_imageID_list: #check\n",
    "            try:\n",
    "                image = Image.open(self.input_path + image_id + '.jpg')\n",
    "                tensor_gt = self.return_tensor_gt_optimized(\n",
    "                    gt_info_dic=self.gt_json['files'][image_id], \n",
    "                    image=image\n",
    "                )\n",
    "                self.precomputed_gt[image_id] = tensor_gt\n",
    "            except Exception as e:\n",
    "                print(f\"Error precomputing {image_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "    def load_GT_json(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"jsonデータを読み込みました。\")\n",
    "        return data\n",
    "\n",
    "    def return_tensor_gt_optimized(self, gt_info_dic, image):\n",
    "        \"\"\"最適化された正解データ生成メソッド\"\"\"\n",
    "        w, h = image.size\n",
    "        \n",
    "        # GPUでテンソルを直接作成\n",
    "        canvas_tensors = torch.zeros(4, h, w, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # 各チャネルを並列処理\n",
    "        channel_names = ['main_region', 'main_affinity', 'furi_region', 'furi_affinity']\n",
    "        \n",
    "        for i, channel_name in enumerate(channel_names):\n",
    "            if channel_name in gt_info_dic and gt_info_dic[channel_name]:\n",
    "                canvas_tensors[i] = self.design_gaussian_map_gpu(\n",
    "                    canvas_tensors[i], \n",
    "                    gt_info_dic[channel_name], \n",
    "                    w, h\n",
    "                )\n",
    "        \n",
    "        return canvas_tensors\n",
    "\n",
    "    def design_gaussian_map_gpu(self, canvas_tensor, point_list, width, height):\n",
    "        \"\"\"GPU上でガウス分布マップを生成\"\"\"\n",
    "        if not point_list:\n",
    "            return canvas_tensor\n",
    "            \n",
    "        # バッチ処理のためにポイントリストを整理\n",
    "        batch_points = torch.tensor(point_list, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        for points in batch_points:\n",
    "            p1x, p1y, p2x, p2y, p3x, p3y, p4x, p4y = points\n",
    "            \n",
    "            # 四角形の各頂点\n",
    "            src_points = torch.tensor([\n",
    "                [p1x, p1y], [p2x, p2y], [p3x, p3y], [p4x, p4y]\n",
    "            ], dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            canvas_tensor = self.add_perspective_gaussian_gpu(\n",
    "                canvas_tensor, src_points, width, height\n",
    "            )\n",
    "        \n",
    "        return canvas_tensor\n",
    "\n",
    "    def add_perspective_gaussian_gpu(self, canvas, src_points, canvas_width, canvas_height):\n",
    "        \"\"\"GPU上で透視変換されたガウス分布を追加\"\"\"\n",
    "        # 四角形のサイズを計算\n",
    "        width = max(\n",
    "            torch.norm(src_points[0] - src_points[1]).item(),\n",
    "            torch.norm(src_points[2] - src_points[3]).item()\n",
    "        )\n",
    "        height = max(\n",
    "            torch.norm(src_points[0] - src_points[3]).item(),\n",
    "            torch.norm(src_points[1] - src_points[2]).item()\n",
    "        )\n",
    "        \n",
    "        width = int(width) + 1\n",
    "        height = int(height) + 1\n",
    "        \n",
    "        # ガウス分布を生成\n",
    "        gaussian = self.create_gaussian_kernel_gpu(width, height)\n",
    "        \n",
    "        # 透視変換行列を計算（CPUで実行）\n",
    "        src_np = src_points.cpu().numpy()\n",
    "        dst_np = np.array([\n",
    "            [0, 0], [width-1, 0], [width-1, height-1], [0, height-1]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        try:\n",
    "            matrix = cv2.getPerspectiveTransform(dst_np, src_np)\n",
    "            \n",
    "            # 変換をGPU上で実行\n",
    "            transformed_gaussian = self.warp_perspective_gpu(\n",
    "                gaussian, matrix, canvas_width, canvas_height\n",
    "            )\n",
    "            \n",
    "            canvas += transformed_gaussian\n",
    "            \n",
    "        except cv2.error:\n",
    "            # 透視変換が失敗した場合はスキップ\n",
    "            pass\n",
    "            \n",
    "        return canvas\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    def create_gaussian_kernel_gpu(self, width, height):\n",
    "        \"\"\"GPU上でガウシアンカーネルを生成（キャッシュ付き）\"\"\"\n",
    "        x = torch.linspace(-width/2, width/2, width, device=self.device)\n",
    "        y = torch.linspace(-height/2, height/2, height, device=self.device)\n",
    "        \n",
    "        # メッシュグリッドを作成\n",
    "        y_grid, x_grid = torch.meshgrid(y, x, indexing='ij')\n",
    "        \n",
    "        # ガウス分布のパラメータ\n",
    "        sigma_x = width / 5.0\n",
    "        sigma_y = height / 5.0\n",
    "        \n",
    "        # ガウス分布を計算\n",
    "        gaussian = torch.exp(-(x_grid**2 / (2 * sigma_x**2) + y_grid**2 / (2 * sigma_y**2)))\n",
    "        \n",
    "        return gaussian\n",
    "\n",
    "    def warp_perspective_gpu(self, image_tensor, matrix, output_width, output_height):\n",
    "        \"\"\"GPU上で透視変換を実行\"\"\"\n",
    "        # 変換行列をテンソルに変換\n",
    "        matrix_tensor = torch.from_numpy(matrix).float().to(self.device)\n",
    "        \n",
    "        # グリッドを生成\n",
    "        grid = self.create_transformation_grid(\n",
    "            matrix_tensor, output_height, output_width\n",
    "        )\n",
    "        \n",
    "        # grid_sampleを使用して変換\n",
    "        image_batch = image_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
    "        grid_batch = grid.unsqueeze(0)  # [1, H, W, 2]\n",
    "        \n",
    "        transformed = F.grid_sample(\n",
    "            image_batch, grid_batch, \n",
    "            mode='bilinear', \n",
    "            padding_mode='zeros',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return transformed.squeeze(0).squeeze(0)\n",
    "\n",
    "    def create_transformation_grid(self, matrix, height, width):\n",
    "        \"\"\"変換グリッドを作成\"\"\"\n",
    "        # 出力座標を生成\n",
    "        y_coords = torch.arange(height, dtype=torch.float32, device=self.device)\n",
    "        x_coords = torch.arange(width, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "        \n",
    "        # 同次座標に変換\n",
    "        ones = torch.ones_like(x_grid)\n",
    "        coords = torch.stack([x_grid, y_grid, ones], dim=-1)  # [H, W, 3]\n",
    "        \n",
    "        # 逆変換行列を適用\n",
    "        try:\n",
    "            inv_matrix = torch.inverse(matrix)\n",
    "        except:\n",
    "            # 逆行列が計算できない場合は単位行列を使用\n",
    "            inv_matrix = torch.eye(3, device=self.device)\n",
    "        \n",
    "        # 変換を適用\n",
    "        transformed_coords = torch.matmul(coords, inv_matrix.T)  # [H, W, 3]\n",
    "        \n",
    "        # 正規化座標に変換\n",
    "        x_norm = transformed_coords[..., 0] / transformed_coords[..., 2]\n",
    "        y_norm = transformed_coords[..., 1] / transformed_coords[..., 2]\n",
    "        \n",
    "        # grid_sampleの座標系に変換 [-1, 1]\n",
    "        grid_x = 2.0 * x_norm / (width - 1) - 1.0\n",
    "        grid_y = 2.0 * y_norm / (height - 1) - 1.0\n",
    "        \n",
    "        grid = torch.stack([grid_x, grid_y], dim=-1)\n",
    "        \n",
    "        return grid\n",
    "\n",
    "\n",
    "# 使用例とパフォーマンス比較\n",
    "def benchmark_dataset(dataset, num_samples=10):\n",
    "    \"\"\"データセットのパフォーマンスをベンチマーク\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        image, gt = dataset[i]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_samples\n",
    "    print(f\"Average time per sample: {avg_time:.4f} seconds\")\n",
    "    return avg_time\n",
    "\n",
    "\n",
    "# データローダー用の高速化設定\n",
    "def create_optimized_dataloader(dataset, batch_size=8, num_workers=4):\n",
    "    \"\"\"最適化されたDataLoaderを作成\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=2 if num_workers > 0 else 2\n",
    "    )"
   ]
>>>>>>> f29692695782fab53c7bfc373e40e90499c5fb1e
  },
  {
   "cell_type": "markdown",
   "id": "9c611ba8",
   "metadata": {},
   "source": [
    "# メインの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "Pre-computing ground truth data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# --- Dataset ---\u001b[39;00m\n\u001b[0;32m     24\u001b[0m test_doc_id_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100241706\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249371\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249376\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249416\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249476\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249537\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003076\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003803\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003967\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200004107\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 25\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(test_doc_id_list,\n\u001b[0;32m     26\u001b[0m                             test_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m                             \u001b[38;5;66;03m# input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\u001b[39;00m\n\u001b[0;32m     28\u001b[0m                             input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/input_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m                             json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m                             transform \u001b[38;5;241m=\u001b[39m transform)\n\u001b[0;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(test_doc_id_list,\n\u001b[0;32m     32\u001b[0m                             test_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m                             \u001b[38;5;66;03m# input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\u001b[39;00m\n\u001b[0;32m     34\u001b[0m                             input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/input_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m                             json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     36\u001b[0m                             transform \u001b[38;5;241m=\u001b[39m transform)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 最適化されたDataLoaderの作成\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mPreTrainDataset.__init__\u001b[1;34m(self, test_doc_id_list, test_mode, input_path, json_path, transform, image_downsample_rate, device, precompute_gt, num_workers)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precompute_gt:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-computing ground truth data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precompute_ground_truth()\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-computation completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m, in \u001b[0;36mPreTrainDataset._precompute_ground_truth\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_path \u001b[38;5;241m+\u001b[39m image_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m     tensor_gt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_tensor_gt_optimized(\n\u001b[0;32m    105\u001b[0m         gt_info_dic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m'\u001b[39m][image_id], \n\u001b[0;32m    106\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed_gt[image_id] \u001b[38;5;241m=\u001b[39m tensor_gt\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[3], line 131\u001b[0m, in \u001b[0;36mPreTrainDataset.return_tensor_gt_optimized\u001b[1;34m(self, gt_info_dic, image)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, channel_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(channel_names):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m channel_name \u001b[38;5;129;01min\u001b[39;00m gt_info_dic \u001b[38;5;129;01mand\u001b[39;00m gt_info_dic[channel_name]:\n\u001b[1;32m--> 131\u001b[0m         canvas_tensors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_gaussian_map_gpu(\n\u001b[0;32m    132\u001b[0m             canvas_tensors[i], \n\u001b[0;32m    133\u001b[0m             gt_info_dic[channel_name], \n\u001b[0;32m    134\u001b[0m             w, h\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canvas_tensors\n",
      "Cell \u001b[1;32mIn[3], line 155\u001b[0m, in \u001b[0;36mPreTrainDataset.design_gaussian_map_gpu\u001b[1;34m(self, canvas_tensor, point_list, width, height)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# 四角形の各頂点\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     src_points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[0;32m    152\u001b[0m         [p1x, p1y], [p2x, p2y], [p3x, p3y], [p4x, p4y]\n\u001b[0;32m    153\u001b[0m     ], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 155\u001b[0m     canvas_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_perspective_gaussian_gpu(\n\u001b[0;32m    156\u001b[0m         canvas_tensor, src_points, width, height\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canvas_tensor\n",
      "Cell \u001b[1;32mIn[3], line 189\u001b[0m, in \u001b[0;36mPreTrainDataset.add_perspective_gaussian_gpu\u001b[1;34m(self, canvas, src_points, canvas_width, canvas_height)\u001b[0m\n\u001b[0;32m    186\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetPerspectiveTransform(dst_np, src_np)\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# 変換をGPU上で実行\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     transformed_gaussian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarp_perspective_gpu(\n\u001b[0;32m    190\u001b[0m         gaussian, matrix, canvas_width, canvas_height\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m     canvas \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformed_gaussian\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 透視変換が失敗した場合はスキップ\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 222\u001b[0m, in \u001b[0;36mPreTrainDataset.warp_perspective_gpu\u001b[1;34m(self, image_tensor, matrix, output_width, output_height)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"GPU上で透視変換を実行\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# 変換行列をテンソルに変換\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m matrix_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(matrix)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# グリッドを生成\u001b[39;00m\n\u001b[0;32m    225\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_transformation_grid(\n\u001b[0;32m    226\u001b[0m     matrix_tensor, output_height, output_width\n\u001b[0;32m    227\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = False,\n",
    "                            # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "                            json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "                            transform = transform)\n",
    "test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "                            test_mode = True,\n",
    "                            # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                            input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "                            json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "                            transform = transform)\n",
    "\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "test_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "# --- データセットとデータローダの準備 ---\n",
    "# train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 学習ループの拡張 ---\n",
    "\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "Using device: cuda\n",
      "jsonデータを読み込みました。\n",
      "学習を開始します...\n",
      "start epcoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]:   0%|          | 0/4965 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "# train_dataset = PreTrainDataset(test_doc_id_list,\n",
    "#                             test_mode = False,\n",
    "#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "#                             input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "#                             json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "#                             transform = transform)\n",
    "# test_dataset = PreTrainDataset(test_doc_id_list,\n",
    "#                             test_mode = True,\n",
    "#                             # input_path = '../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "#                             input_path = '../kuzushiji-recognition/synthetic_images_backup/input_images/',\n",
    "#                             json_path = '../kuzushiji-recognition/synthetic_images_backup/gt_json_backup.json',\n",
    "#                             transform = transform)\n",
    "\n",
    "train_dataset = PreTrainDataset(\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=False,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=False,  # 事前計算を有効化\n",
    "    # num_workers=None\n",
    ")\n",
    "test_dataset = PreTrainDataset(\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=True,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=False,  # 事前計算を有効化\n",
    "    # num_workers=4\n",
    ")\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "test_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "# --- データセットとデータローダの準備 ---\n",
    "# train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 学習ループの拡張 ---\n",
    "\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuzushiji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
