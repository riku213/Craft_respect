{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990d7475",
   "metadata": {},
   "source": [
    "# MyDatasetの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb80058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import functional\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "class PreTrainDataset_old(Dataset):\n",
    "    def __init__(self,\n",
    "                 test_doc_id_list,\n",
    "                 test_mode=False,\n",
    "                 # input_path='../kuzushiji_recognition/synthetic_images/input_images/',\n",
    "                 input_path='../kuzushiji_recognition/synthetic_images/tmp_entire_data/',\n",
    "                 json_path='../kuzushiji_recognition/synthetic_images/gt_json.json',\n",
    "                 transform=None,\n",
    "                 target_width=300):  # デフォルトの横幅を300に設定\n",
    "        super().__init__()\n",
    "        self.test_doc_id_list = test_doc_id_list\n",
    "        self.input_path = input_path\n",
    "        self.transform = transform\n",
    "        self.target_width = target_width\n",
    "        \n",
    "        # 画像のIDをリストにして保管。\n",
    "        self.input_imageID_list = []\n",
    "        for file_name in os.listdir(self.input_path):\n",
    "            file_path = os.path.join(self.input_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                # test_modeに応じて、テスト用データと学習用データを切り分ける\n",
    "                # XOR (^) を利用:\n",
    "                # - test_mode=False (学習時): doc_idがtest_listにない(False) -> 全体はFalse -> 読み込む\n",
    "                # - test_mode=True (テスト時): doc_idがtest_listにある(True) -> 全体はFalse -> 読み込む\n",
    "                if not (file_name.split('_sep_')[0] in self.test_doc_id_list) ^ test_mode:\n",
    "                    self.input_imageID_list.append(file_name.split('.')[0])\n",
    "        \n",
    "        # アノテーションデータを保持するjsonファイルをロード\n",
    "        self.gt_json = self.load_GT_json(json_path)\n",
    "\n",
    "        # 入力画像に対応するアノテーションデータが存在するか確認。(コメントアウト)\n",
    "        # for i in range(len(self.input_imageID_list)-1, -1,-1):\n",
    "        #     if not self.input_imageID_list[i] in self.gt_json:\n",
    "        #         del self.input_imageID_list[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_imageID_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.input_imageID_list[index]\n",
    "        image = Image.open(self.input_path + image_id + '.jpg')\n",
    "        \n",
    "        # 1. 元の画像のサイズを取得\n",
    "        original_w, original_h = image.size\n",
    "        \n",
    "        # 2. アスペクト比を保持したまま、横幅を指定サイズにリサイズ\n",
    "        aspect_ratio = original_h / original_w\n",
    "        new_w = self.target_width\n",
    "        new_h = int(self.target_width * aspect_ratio)\n",
    "        new_size = (new_h, new_w)  # functional.resizeはH,Wの順で指定\n",
    "        \n",
    "        # 3. 画像をリサイズ\n",
    "        image = functional.resize(image, new_size, interpolation=functional.InterpolationMode.BILINEAR)\n",
    "        \n",
    "        # 4. リサイズ後のサイズで正解マップを生成\n",
    "        tensor_gt = self.return_tensor_gt(\n",
    "            gt_info_dic=self.gt_json['files'][image_id],\n",
    "            image=image,\n",
    "            original_size=(original_w, original_h)\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, tensor_gt\n",
    "\n",
    "    def load_GT_json(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"json データを読み込みました。\")\n",
    "        return data\n",
    "\n",
    "    def return_ground_truth_canvas(self, image):\n",
    "        w, h = image.size\n",
    "        main_region = np.zeros((h, w), dtype=np.float64)\n",
    "        main_affinity = np.zeros((h, w), dtype=np.float64)\n",
    "        furi_region = np.zeros((h, w), dtype=np.float64)\n",
    "        furi_affinity = np.zeros((h, w), dtype=np.float64)\n",
    "        return main_region, main_affinity, furi_region, furi_affinity\n",
    "\n",
    "    def add_perspective_gaussian_to_canvas(self, canvas, points, amplitude=1.0):\n",
    "        # 領域の4点を取得\n",
    "        src_points = np.array(points, dtype=np.float32)\n",
    "\n",
    "        # ガウス分布を生成するための仮想的な正方形領域を定義\n",
    "        width = int(max(np.linalg.norm(src_points[0] - src_points[1]), np.linalg.norm(src_points[2] - src_points[3])))\n",
    "        height = int(max(np.linalg.norm(src_points[0] - src_points[3]), np.linalg.norm(src_points[1] - src_points[2])))\n",
    "        \n",
    "        # 最小サイズを保証（1ピクセル未満にならないようにする）\n",
    "        width = max(width, 1)\n",
    "        height = max(height, 1)\n",
    "        \n",
    "        # ガウス分布のサイズを調整（小さすぎる場合は大きくする）\n",
    "        min_gaussian_size = 5\n",
    "        if width < min_gaussian_size or height < min_gaussian_size:\n",
    "            scale = max(min_gaussian_size / width, min_gaussian_size / height)\n",
    "            width = max(int(width * scale), min_gaussian_size)\n",
    "            height = max(int(height * scale), min_gaussian_size)\n",
    "        \n",
    "        dst_points = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=np.float32)\n",
    "\n",
    "        # ガウス分布を生成\n",
    "        x = np.linspace(-width / 2, width / 2, width)\n",
    "        y = np.linspace(-height / 2, height / 2, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        \n",
    "        # シグマ値の調整（小さすぎる場合は最小値を設定）\n",
    "        min_sigma = 1.0\n",
    "        sigma_x = max(width / 5.0, min_sigma)\n",
    "        sigma_y = max(height / 5.0, min_sigma)\n",
    "        \n",
    "        gaussian = amplitude * np.exp(-((x**2) / (2 * sigma_x**2) + (y**2) / (2 * sigma_y**2)))\n",
    "\n",
    "        try:\n",
    "            # Perspective Transformation行列を計算\n",
    "            matrix = cv2.getPerspectiveTransform(dst_points, src_points)\n",
    "\n",
    "            # 入力ガウシアンを適切な形式に変換\n",
    "            gaussian = gaussian.astype(np.float32)  # float32型に変換\n",
    "            \n",
    "            # ガウス分布をPerspective Transformationで変形\n",
    "            transformed_gaussian = cv2.warpPerspective(\n",
    "                gaussian,\n",
    "                matrix,\n",
    "                (canvas.shape[1], canvas.shape[0]),\n",
    "                flags=cv2.INTER_LINEAR,\n",
    "                borderMode=cv2.BORDER_CONSTANT,\n",
    "                borderValue=0\n",
    "            )\n",
    "\n",
    "            # キャンバスにガウス分布を追加\n",
    "            canvas += transformed_gaussian\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: perspective transformation failed - {e}\")\n",
    "            # エラーが発生した場合は、キャンバスをそのまま返す\n",
    "\n",
    "        return canvas\n",
    "\n",
    "    def design_gaussian_map(self, canvas, point_list):\n",
    "        for points in point_list:\n",
    "            p1x, p1y, p2x, p2y, p3x, p3y, p4x, p4y = points\n",
    "            self.add_perspective_gaussian_to_canvas(canvas, ((p1x, p1y), (p2x, p2y), (p3x, p3y), (p4x, p4y)), amplitude=1.0)\n",
    "        return canvas\n",
    "\n",
    "    def return_tensor_gt(self, gt_info_dic, image, original_size=None):\n",
    "        \"\"\"\n",
    "        正解マップを生成する\n",
    "        Args:\n",
    "            gt_info_dic: アノテーションデータ\n",
    "            image: リサイズ済みの画像\n",
    "            original_size: 元の画像サイズ（width, height）\n",
    "        \"\"\"\n",
    "        main_region, main_affinity, furi_region, furi_affinity = self.return_ground_truth_canvas(image)\n",
    "\n",
    "        # スケーリング係数を計算\n",
    "        if original_size:\n",
    "            orig_w, orig_h = original_size\n",
    "            current_w, current_h = image.size\n",
    "            scale_w = current_w / orig_w\n",
    "            scale_h = current_h / orig_h\n",
    "        else:\n",
    "            scale_w = scale_h = 1.0\n",
    "\n",
    "        # キャンバスマップを作成\n",
    "        canvas_list = []\n",
    "        canvas_map = {\n",
    "            'main_region': main_region,\n",
    "            'main_affinity': main_affinity,\n",
    "            'furi_region': furi_region,\n",
    "            'furi_affinity': furi_affinity\n",
    "        }\n",
    "        \n",
    "        for name, canvas in canvas_map.items():\n",
    "            if name in gt_info_dic:\n",
    "                # 座標をスケーリング\n",
    "                scaled_points = []\n",
    "                for points in gt_info_dic[name]:\n",
    "                    scaled_point = []\n",
    "                    for i, coord in enumerate(points):\n",
    "                        # 偶数インデックスはx座標、奇数インデックスはy座標\n",
    "                        scaled_coord = coord * (scale_w if i % 2 == 0 else scale_h)\n",
    "                        scaled_point.append(scaled_coord)\n",
    "                    scaled_points.append(scaled_point)\n",
    "                \n",
    "                canvas_list.append(self.design_gaussian_map(canvas, scaled_points))\n",
    "\n",
    "        # それぞれをtorch tensorに変換\n",
    "        tensor_list = []\n",
    "        for canvas in canvas_list:\n",
    "            tensor_list.append(torch.from_numpy(canvas.astype(np.float32)).clone())\n",
    "        \n",
    "        return_tensor = torch.stack(tensor_list)\n",
    "        return return_tensor\n",
    "\n",
    "    def create_optimized_dataloader_for_old_dataset(dataset, batch_size=8, num_workers=4):\n",
    "        \"\"\"\n",
    "        PreTrainDataset_old用の最適化されたDataLoaderを作成\n",
    "        \n",
    "        Args:\n",
    "            dataset: PreTrainDataset_oldのインスタンス\n",
    "            batch_size (int): バッチサイズ\n",
    "            num_workers (int): データローディングに使用するワーカープロセスの数\n",
    "        \n",
    "        Returns:\n",
    "            DataLoader: 最適化されたDataLoader\n",
    "        \"\"\"\n",
    "        from torch.utils.data import DataLoader\n",
    "        \n",
    "        # メモリ効率のための設定\n",
    "        persistent_workers = num_workers > 0\n",
    "        prefetch_factor = 2 if persistent_workers else None\n",
    "        \n",
    "        # DataLoaderの作成\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            # メモリ効率のための追加設定\n",
    "            drop_last=False,  # データセットの最後のバッチも使用\n",
    "            generator=torch.Generator(),  # 再現性のための乱数生成器\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7944f",
   "metadata": {},
   "source": [
    "# MyDatasetオブジェクト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e01f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../kuzushiji-recognition/synthetic_images/input_images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m test_doc_id_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100241706\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249371\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249376\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249416\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249476\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249537\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003076\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003803\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003967\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200004107\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     13\u001b[0m ])\n\u001b[0;32m---> 14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainDataset_old\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../kuzushiji-recognition/synthetic_images/input_images/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../kuzushiji-recognition/synthetic_images/gt_json.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_doc_id_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_doc_id_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 横幅を300ピクセルに固定\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mPreTrainDataset_old.__init__\u001b[0;34m(self, test_doc_id_list, test_mode, input_path, json_path, transform, target_width)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 画像のIDをリストにして保管。\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_imageID_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     30\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_path, file_name)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# test_modeに応じて、テスト用データと学習用データを切り分ける\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# XOR (^) を利用:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# - test_mode=False (学習時): doc_idがtest_listにない(False) -> 全体はFalse -> 読み込む\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# - test_mode=True (テスト時): doc_idがtest_listにある(True) -> 全体はFalse -> 読み込む\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../kuzushiji-recognition/synthetic_images/input_images/'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = PreTrainDataset_old(\n",
    "    input_path='../../kuzushiji-recognition/synthetic_images/input_images/',\n",
    "    json_path='../../kuzushiji-recognition/synthetic_images/gt_json.json',\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=False,\n",
    "    transform=transform,\n",
    "    target_width=300  # 横幅を300ピクセルに固定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4819d",
   "metadata": {},
   "source": [
    "# 正解データの表示と時間計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47281140",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m start_memory \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m  \u001b[38;5;66;03m# MB単位\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m first_data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[i]\n\u001b[1;32m     13\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m end_memory \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "for i in range(10):\n",
    "    start_time = time.time()\n",
    "    start_memory = process.memory_info().rss / 1024 / 1024  # MB単位\n",
    "\n",
    "    first_data = dataset[i]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Memory change: {end_memory - start_memory:.2f} MB\")\n",
    "    print(f\"Output tensor shapes:\")\n",
    "    print(f\"Input image: {first_data[0].shape}\")\n",
    "    for j in range(4):\n",
    "        print(f\"GT channel {j}: {first_data[1][j].shape}\")\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(first_data[1][j])\n",
    "        plt.title(f\"Channel {j}\")\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e796d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
