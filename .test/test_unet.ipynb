{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1312834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from src.my_app import UNet, PreTrainDataset, create_optimized_dataloader\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../kuzushiji-recognition/synthetic_images/input_images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# --- Dataset ---\u001b[39;00m\n\u001b[1;32m     26\u001b[0m test_doc_id_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100241706\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249371\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249376\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249416\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249476\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100249537\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003076\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003803\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200003967\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200004107\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../kuzushiji-recognition/synthetic_images/input_images/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../kuzushiji-recognition/synthetic_images/gt_json.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_doc_id_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_doc_id_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPUを明示的に指定\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecompute_gt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 事前計算を有効化\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# num_workers=None\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 画像変換を追加\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PreTrainDataset(\n\u001b[1;32m     39\u001b[0m     input_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../kuzushiji-recognition/synthetic_images/input_images/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m     json_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../kuzushiji-recognition/synthetic_images/gt_json.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     targetwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 最適化されたDataLoaderの作成\u001b[39;00m\n",
      "File \u001b[0;32m~/00_myprograms/Craft_respect/src/my_app/core/MyDataset/MyDataset.py:47\u001b[0m, in \u001b[0;36mPreTrainDataset.__init__\u001b[0;34m(self, test_doc_id_list, test_mode, input_path, json_path, transform, target_width, device, precompute_gt, num_workers)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 画像のIDをリストにして保管\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_imageID_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     48\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_path, file_name)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../kuzushiji-recognition/synthetic_images/input_images/'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def crop_labels_to_match(labels_to_crop, target_tensor):\n",
    "    target_h, target_w = target_tensor.shape[2:]\n",
    "    source_h, source_w = labels_to_crop.shape[2:]\n",
    "    delta_h = (source_h - target_h) // 2\n",
    "    delta_w = (source_w - target_w) // 2\n",
    "    return labels_to_crop[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resizeze((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# --- Dataset ---\n",
    "test_doc_id_list = ['100241706', '100249371', '100249376', '100249416', '100249476', '100249537', '200003076', '200003803', '200003967', '200004107']\n",
    "train_dataset = PreTrainDataset(\n",
    "    input_path='../../kuzushiji-recognition/synthetic_images/input_images/',\n",
    "    json_path='../../kuzushiji-recognition/synthetic_images/gt_json.json',\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=False,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=True,  # 事前計算を有効化\n",
    "    # num_workers=None\n",
    "    transform=transform,  # 画像変換を追加\n",
    "    target_width=300\n",
    ")\n",
    "test_dataset = PreTrainDataset(\n",
    "    input_path='../../kuzushiji-recognition/synthetic_images/input_images/',\n",
    "    json_path='../../kuzushiji-recognition/synthetic_images/gt_json.json',\n",
    "    test_doc_id_list=test_doc_id_list,\n",
    "    test_mode=True,\n",
    "    device=torch.device('cuda'),  # GPUを明示的に指定\n",
    "    precompute_gt=True,  # 事前計算を有効化\n",
    "    # num_workers=4\n",
    "    transform=transform,  # 画像変換を追加\n",
    "    targetwidth=300\n",
    ")\n",
    "\n",
    "# 最適化されたDataLoaderの作成\n",
    "train_dl = create_optimized_dataloader(train_dataset, batch_size=1, num_workers=min(mp.cpu_count(), 4))\n",
    "test_dl = create_optimized_dataloader(test_dataset, batch_size=1, num_workers=min(mp.cpu_count(), 4))\n",
    "\n",
    "# --- モデル、損失関数、最適化手法の定義 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- チェックポイントの設定 ---\n",
    "checkpoint_dir = \"../.checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 最良のモデルを追跡するための変数\n",
    "best_test_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# チェックポイントの読み込み（存在する場合）\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_test_loss = checkpoint['best_test_loss']\n",
    "    train_loss_history = checkpoint['train_loss_history']\n",
    "    test_loss_history = checkpoint['test_loss_history']\n",
    "    print(f\"チェックポイントを読み込みました（エポック {start_epoch}）\")\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "num_epochs = 100 # エポック数を定義\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --- 評価フェーズ ---\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    test_loss_total = 0\n",
    "    \n",
    "    # 勾配計算を無効化して、メモリ効率を良くする\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
    "        for imgs, masks in test_bar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            cropped_masks = crop_labels_to_match(masks, preds)\n",
    "            \n",
    "            loss = criterion(preds, cropped_masks)\n",
    "            test_loss_total += loss.item()\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_test_loss = test_loss_total / len(test_dl)\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    \n",
    "    # 各エポックの最後に訓練ロスとテストロスを表示\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    # 最新のチェックポイントを保存\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss_history': train_loss_history,\n",
    "        'test_loss_history': test_loss_history,\n",
    "        'best_test_loss': best_test_loss\n",
    "    }, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "    \n",
    "    # より良い性能が出た場合、ベストモデルとして保存\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_loss': avg_test_loss,\n",
    "        }, os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
    "        print(f\"新しいベストモデルを保存しました（Test Loss: {avg_test_loss:.4f}）\")\n",
    "\n",
    "print(\"学習が完了しました。\")\n",
    "\n",
    "# --- 損失の推移をグラフで表示 ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.title(\"Loss Trend\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d51772",
   "metadata": {},
   "source": [
    "# 学習済みモデルの出力を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb0a9f",
   "metadata": {},
   "source": [
    "## チェックポイントのロード動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "チェックポイントを読み込みました: ../.checkpoints/latest_checkpoint.pth\n",
      "エポック: 87\n",
      "ベストテストロス: 82.97725561295073\n",
      "訓練ロス履歴の長さ: 87\n",
      "テストロス履歴の長さ: 87\n"
     ]
    }
   ],
   "source": [
    "# --- チェックポイントの設定 ---.checkpoints/latest_checkpoint.pth\n",
    "checkpoint_dir = '../.checkpoints'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "\n",
    "# 現在のデバイス状況を確認\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # デバイス間の互換性を保つための読み込み\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    print(f\"チェックポイントを読み込みました: {checkpoint_path}\")\n",
    "    print(f\"エポック: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"ベストテストロス: {checkpoint.get('best_test_loss', 'N/A')}\")\n",
    "    print(f\"訓練ロス履歴の長さ: {len(checkpoint.get('train_loss_history', []))}\")\n",
    "    print(f\"テストロス履歴の長さ: {len(checkpoint.get('test_loss_history', []))}\")\n",
    "else:\n",
    "    print(f\"チェックポイントが見つかりません: {checkpoint_path}\")\n",
    "    checkpoint = None\n",
    "    checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09ec8719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "チェックポイントを読み込みました（エポック 87）\n",
      "学習を開始します...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m test_loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m学習を開始します...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, \u001b[43mnum_epochs\u001b[49m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart epcoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# --- 訓練フェーズ ---\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# --- チェックポイントの設定 ---.checkpoints/latest_checkpoint.pth\n",
    "# 現在のデバイス状況を確認\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3, 4).to(device)\n",
    "criterion = nn.MSELoss() # 回帰問題なのでMSE損失を使用\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- チェックポイントの設定 ---\n",
    "checkpoint_dir = \"../.checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 最良のモデルを追跡するための変数\n",
    "best_test_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# チェックポイントの読み込み（存在する場合）\n",
    "# セル6のチェックポイント読み込み部分を修正\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # CUDAからMPS/CPUへの変換を明示的に行う\n",
    "    if device.type == 'mps':\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('mps'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # OptimizerもCPU/MPSに適応させる必要がある場合がある\n",
    "    try:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Optimizer状態の読み込みに失敗: {e}\")\n",
    "        print(\"新しいOptimizerで続行します\")\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_test_loss = checkpoint['best_test_loss']\n",
    "    train_loss_history = checkpoint['train_loss_history']\n",
    "    test_loss_history = checkpoint['test_loss_history']\n",
    "    print(f\"チェックポイントを読み込みました（エポック {start_epoch}）\")\n",
    "\n",
    "# 損失の履歴を保存するリストを初期化\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(\"学習を開始します...\")\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f'start epcoch')\n",
    "    # --- 訓練フェーズ ---\n",
    "    model.train() # モデルを訓練モードに設定\n",
    "    train_loss_total = 0\n",
    "    \n",
    "    # tqdmでプログレスバーを表示\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in train_bar:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        cropped_masks = crop_labels_to_match(masks, preds)\n",
    "\n",
    "        loss = criterion(preds, cropped_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_total += loss.item()\n",
    "        # プログレスバーに現在のロスを表示\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss_total / len(train_dl)\n",
    "    train_loss_history.append(avg_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6cc3b",
   "metadata": {},
   "source": [
    "## 画像読み込みと torch tensor 変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_as_tensor(image_path, device=None, target_size=None, normalize=True):\n",
    "    \"\"\"\n",
    "    画像ファイルを読み込んでtorch.tensorに変換する\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 画像ファイルのパス\n",
    "        device (torch.device, optional): テンソルを配置するデバイス。Noneの場合は自動選択\n",
    "        target_size (tuple, optional): リサイズするサイズ (height, width)\n",
    "        normalize (bool): 正規化を行うかどうか (0-1の範囲に変換)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: 変換された画像テンソル (C, H, W)\n",
    "    \"\"\"\n",
    "    \n",
    "    # デバイスの自動選択\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # ファイルの存在確認\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"画像ファイルが見つかりません: {image_path}\")\n",
    "    \n",
    "    try:\n",
    "        # 画像を読み込み\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # RGBに変換（PNGのアルファチャンネルなどを除去）\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # 変換パイプラインを構築\n",
    "        transform_list = []\n",
    "        \n",
    "        # リサイズ\n",
    "        if target_size is not None:\n",
    "            transform_list.append(transforms.Resize(target_size))\n",
    "        \n",
    "        # テンソル変換\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "        \n",
    "        # 正規化（ToTensorで既に0-1に正規化されるが、ImageNetの標準化も可能）\n",
    "        if normalize == 'imagenet':\n",
    "            transform_list.append(transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ))\n",
    "        \n",
    "        # 変換を実行\n",
    "        transform = transforms.Compose(transform_list)\n",
    "        tensor = transform(image)\n",
    "        \n",
    "        # デバイスに移動\n",
    "        tensor = tensor.to(device)\n",
    "        \n",
    "        return tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"画像の読み込み/変換でエラーが発生しました: {e}\")\n",
    "\n",
    "# 使用例\n",
    "def load_specific_image():\n",
    "    \"\"\"指定された画像を読み込む\"\"\"\n",
    "    image_path = \".image_data/100241706_00002_2.jpg\"\n",
    "    \n",
    "    # デバイス設定\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # 画像をテンソルに変換\n",
    "        image_tensor = load_image_as_tensor(\n",
    "            image_path=image_path,\n",
    "            device=device,\n",
    "            target_size=(300, 300),  # 必要に応じてリサイズ\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        print(f\"画像が正常に読み込まれました\")\n",
    "        print(f\"テンソルの形状: {image_tensor.shape}\")\n",
    "        print(f\"データ型: {image_tensor.dtype}\")\n",
    "        print(f\"値の範囲: {image_tensor.min():.3f} - {image_tensor.max():.3f}\")\n",
    "        print(f\"デバイス: {image_tensor.device}\")\n",
    "        \n",
    "        return image_tensor\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"画像ファイルが見つかりません: {image_path}\")\n",
    "        print(\"現在のディレクトリから相対パスを確認してください\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        return None\n",
    "\n",
    "# バッチ用の関数\n",
    "def add_batch_dimension(tensor):\n",
    "    \"\"\"\n",
    "    テンソルにバッチ次元を追加 (C, H, W) -> (1, C, H, W)\n",
    "    \"\"\"\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "# 実行例\n",
    "if __name__ == \"__main__\":\n",
    "    # 画像を読み込み\n",
    "    image_tensor = load_specific_image()\n",
    "    \n",
    "    if image_tensor is not None:\n",
    "        # バッチ次元を追加（モデル推論用）\n",
    "        batch_tensor = add_batch_dimension(image_tensor)\n",
    "        print(f\"バッチテンソルの形状: {batch_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93244d",
   "metadata": {},
   "source": [
    "## Dataset型形式確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257d655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc060e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
