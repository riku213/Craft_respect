{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfa7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_and_dirs(path):\n",
    "    try:\n",
    "        items = os.listdir(path)\n",
    "        for item in items:\n",
    "            print(item)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"指定されたパスが見つかりません: {path}\")\n",
    "    except NotADirectoryError:\n",
    "        print(f\"指定されたパスはディレクトリではありません: {path}\")\n",
    "    except PermissionError:\n",
    "        print(f\"指定されたパスへのアクセス権がありません: {path}\")\n",
    "\n",
    "# 使用例\n",
    "# list_files_and_dirs('/path/to/directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3bf95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "100241706\n"
     ]
    }
   ],
   "source": [
    "list_files_and_dirs('../../../../kuzushiji-recognition/char_sep_datas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7718ff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 画像とアノテーションデータの対応調査 ===\n",
      "\n",
      "画像ファイル数: 76\n",
      "最初の5つの画像ID: ['100241706_sep_100241706_00027_1', '100241706_sep_100241706_00018_2', '100241706_sep_100241706_00003_1', '100241706_sep_100241706_00025_2', '100241706_sep_100241706_00027_2']\n",
      "\n",
      "JSONファイル内のID数: 76\n",
      "最初の5つのJSONファイルID: ['100241706_sep_100241706_00003_1', '100241706_sep_100241706_00025_2', '100241706_sep_100241706_00018_2', '100241706_sep_100241706_00027_1', '100241706_sep_100241706_00025_1']\n",
      "\n",
      "=== ID形式の分析 ===\n",
      "画像ファイルIDの例:\n",
      "  1: '100241706_sep_100241706_00027_1'\n",
      "  2: '100241706_sep_100241706_00018_2'\n",
      "  3: '100241706_sep_100241706_00003_1'\n",
      "  4: '100241706_sep_100241706_00025_2'\n",
      "  5: '100241706_sep_100241706_00027_2'\n",
      "\n",
      "JSONファイルIDの例:\n",
      "  1: '100241706_sep_100241706_00003_1'\n",
      "  2: '100241706_sep_100241706_00025_2'\n",
      "  3: '100241706_sep_100241706_00018_2'\n",
      "  4: '100241706_sep_100241706_00027_1'\n",
      "  5: '100241706_sep_100241706_00025_1'\n",
      "\n",
      "=== ID対応関係の確認 ===\n",
      "✓ 画像ID '100241706_sep_100241706_00027_1' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00018_2' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00003_1' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00025_2' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00027_2' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00018_1' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00025_1' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00003_2' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00021_1' → JSON内に存在\n",
      "✓ 画像ID '100241706_sep_100241706_00007_2' → JSON内に存在\n",
      "\n",
      "=== ID変換規則の推測 ===\n",
      "JSONファイルID例: '100241706_sep_100241706_00003_1'\n",
      "  '_sep_'で分割: ['100241706', '100241706_00003_1']\n",
      "  推測される変換: doc_id='100241706', file_part='100241706_00003_1'\n",
      "  ✗ 対応画像ファイル見つからない: '100241706_00003_1'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def investigate_image_annotation_mismatch():\n",
    "    \"\"\"画像とアノテーションデータの対応関係を調査\"\"\"\n",
    "    \n",
    "    # パスの設定\n",
    "    input_images_path = '../../../../kuzushiji-recognition/synthetic_images/input_images/'\n",
    "    json_path = '../../../../kuzushiji-recognition/synthetic_images/gt_json.json'\n",
    "    \n",
    "    print(\"=== 画像とアノテーションデータの対応調査 ===\\n\")\n",
    "    \n",
    "    # 1. 画像ファイル一覧を取得\n",
    "    if os.path.exists(input_images_path):\n",
    "        image_files = glob.glob(os.path.join(input_images_path, \"*.jpg\"))\n",
    "        image_ids = [os.path.basename(f).replace('.jpg', '') for f in image_files]\n",
    "        print(f\"画像ファイル数: {len(image_files)}\")\n",
    "        print(f\"最初の5つの画像ID: {image_ids[:5]}\")\n",
    "    else:\n",
    "        print(f\"画像ディレクトリが見つかりません: {input_images_path}\")\n",
    "        return\n",
    "    \n",
    "    # 2. JSONファイルを読み込み\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        json_file_ids = list(json_data.get('files', {}).keys())\n",
    "        print(f\"\\nJSONファイル内のID数: {len(json_file_ids)}\")\n",
    "        print(f\"最初の5つのJSONファイルID: {json_file_ids[:5]}\")\n",
    "    else:\n",
    "        print(f\"JSONファイルが見つかりません: {json_path}\")\n",
    "        return\n",
    "    \n",
    "    # 3. ID形式の分析\n",
    "    print(\"\\n=== ID形式の分析 ===\")\n",
    "    print(\"画像ファイルIDの例:\")\n",
    "    for i, img_id in enumerate(image_ids[:5]):\n",
    "        print(f\"  {i+1}: '{img_id}'\")\n",
    "    \n",
    "    print(\"\\nJSONファイルIDの例:\")\n",
    "    for i, json_id in enumerate(json_file_ids[:5]):\n",
    "        print(f\"  {i+1}: '{json_id}'\")\n",
    "    \n",
    "    # 4. ID対応関係の確認\n",
    "    print(\"\\n=== ID対応関係の確認 ===\")\n",
    "    \n",
    "    # 画像IDに対応するJSONエントリがあるかチェック\n",
    "    missing_in_json = []\n",
    "    for img_id in image_ids[:10]:  # 最初の10個をチェック\n",
    "        if img_id in json_data.get('files', {}):\n",
    "            print(f\"✓ 画像ID '{img_id}' → JSON内に存在\")\n",
    "        else:\n",
    "            # 類似IDを探す\n",
    "            similar_ids = [jid for jid in json_file_ids if img_id in jid or jid in img_id]\n",
    "            if similar_ids:\n",
    "                print(f\"✗ 画像ID '{img_id}' → JSON内に直接対応なし\")\n",
    "                print(f\"    類似ID: {similar_ids[:3]}\")\n",
    "            else:\n",
    "                print(f\"✗ 画像ID '{img_id}' → JSON内に対応なし\")\n",
    "                missing_in_json.append(img_id)\n",
    "    \n",
    "    # 5. JSONのIDでファイル名変換規則を推測\n",
    "    print(\"\\n=== ID変換規則の推測 ===\")\n",
    "    if json_file_ids:\n",
    "        sample_json_id = json_file_ids[0]\n",
    "        print(f\"JSONファイルID例: '{sample_json_id}'\")\n",
    "        \n",
    "        # '_sep_' が含まれているかチェック\n",
    "        if '_sep_' in sample_json_id:\n",
    "            parts = sample_json_id.split('_sep_')\n",
    "            print(f\"  '_sep_'で分割: {parts}\")\n",
    "            if len(parts) == 2:\n",
    "                doc_id, file_part = parts\n",
    "                print(f\"  推測される変換: doc_id='{doc_id}', file_part='{file_part}'\")\n",
    "                \n",
    "                # 対応する画像ファイルを推測\n",
    "                expected_image_id = file_part  # または別の変換規則\n",
    "                if expected_image_id in image_ids:\n",
    "                    print(f\"  ✓ 対応画像ファイル見つかる: '{expected_image_id}'\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 対応画像ファイル見つからない: '{expected_image_id}'\")\n",
    "    \n",
    "    return {\n",
    "        'image_ids': image_ids,\n",
    "        'json_file_ids': json_file_ids,\n",
    "        'missing_in_json': missing_in_json\n",
    "    }\n",
    "\n",
    "# 調査実行\n",
    "result = investigate_image_annotation_mismatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9116a08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ID生成過程の調査 ===\n",
      "\n",
      "ID生成シミュレーション:\n",
      "  パス: /path/to/100241706/images/100241706_00003_1.jpg\n",
      "  生成ID: '100241706_sep_100241706_00003_1'\n",
      "\n",
      "  パス: /path/to/100241706/images/100241706_00025_2.jpg\n",
      "  生成ID: '100241706_sep_100241706_00025_2'\n",
      "\n",
      "  パス: /path/to/100249371/images/100249371_00001_1.jpg\n",
      "  生成ID: '100249371_sep_100249371_00001_1'\n",
      "\n",
      "=== 並行処理問題の調査 ===\n",
      "\n",
      "doc_id別のファイル数:\n",
      "  100241706: 76個のファイル\n",
      "    - 100241706_sep_100241706_00003_1\n",
      "    - 100241706_sep_100241706_00025_2\n",
      "    - 100241706_sep_100241706_00018_2\n",
      "    ... 他73個\n",
      "\n",
      "=== アノテーションデータの内容チェック ===\n",
      "ファイルID: 100241706_sep_100241706_00003_1\n",
      "  main_region数: 361\n",
      "  main_affinity数: 342\n",
      "  furi_region数: 786\n",
      "  furi_affinity数: 472\n",
      "  最初のregion座標: [222, 430, 347, 430, 347, 549, 222, 549]\n",
      "\n",
      "ファイルID: 100241706_sep_100241706_00025_2\n",
      "  main_region数: 155\n",
      "  main_affinity数: 142\n",
      "  furi_region数: 278\n",
      "  furi_affinity数: 139\n",
      "  最初のregion座標: [393, 521, 509, 521, 509, 621, 393, 621]\n",
      "\n",
      "ファイルID: 100241706_sep_100241706_00018_2\n",
      "  main_region数: 251\n",
      "  main_affinity数: 234\n",
      "  furi_region数: 453\n",
      "  furi_affinity数: 232\n",
      "  最初のregion座標: [220, 757, 315, 757, 315, 849, 220, 849]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def investigate_id_generation_process():\n",
    "    \"\"\"ID生成過程の問題を調査\"\"\"\n",
    "    \n",
    "    print(\"=== ID生成過程の調査 ===\\n\")\n",
    "    \n",
    "    # generate_dateset.pyのID生成ロジックを再現\n",
    "    def simulate_id_generation(file_path):\n",
    "        \"\"\"ID生成ロジックのシミュレーション\"\"\"\n",
    "        doc_id = file_path.split('/')[-3]\n",
    "        file_id = str(doc_id) + '_sep_' + file_path.split('/')[-1].split('.')[0]\n",
    "        return file_id\n",
    "    \n",
    "    # サンプルファイルパスでテスト\n",
    "    sample_paths = [\n",
    "        '/path/to/100241706/images/100241706_00003_1.jpg',\n",
    "        '/path/to/100241706/images/100241706_00025_2.jpg',\n",
    "        '/path/to/100249371/images/100249371_00001_1.jpg'\n",
    "    ]\n",
    "    \n",
    "    print(\"ID生成シミュレーション:\")\n",
    "    for path in sample_paths:\n",
    "        generated_id = simulate_id_generation(path)\n",
    "        print(f\"  パス: {path}\")\n",
    "        print(f\"  生成ID: '{generated_id}'\")\n",
    "        print()\n",
    "    \n",
    "    return\n",
    "\n",
    "def check_concurrent_processing_issues():\n",
    "    \"\"\"並行処理での問題を調査\"\"\"\n",
    "    \n",
    "    print(\"=== 並行処理問題の調査 ===\\n\")\n",
    "    \n",
    "    json_path = '../../../../kuzushiji-recognition/synthetic_images/gt_json.json'\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"JSONファイルが見つかりません: {json_path}\")\n",
    "        return\n",
    "    \n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # 1. 同じdoc_idを持つエントリを分析\n",
    "    doc_id_groups = {}\n",
    "    for file_id in json_data.get('files', {}).keys():\n",
    "        if '_sep_' in file_id:\n",
    "            doc_id = file_id.split('_sep_')[0]\n",
    "            if doc_id not in doc_id_groups:\n",
    "                doc_id_groups[doc_id] = []\n",
    "            doc_id_groups[doc_id].append(file_id)\n",
    "    \n",
    "    print(\"doc_id別のファイル数:\")\n",
    "    for doc_id, file_ids in doc_id_groups.items():\n",
    "        print(f\"  {doc_id}: {len(file_ids)}個のファイル\")\n",
    "        if len(file_ids) <= 5:  # 少ない場合は全て表示\n",
    "            for fid in file_ids:\n",
    "                print(f\"    - {fid}\")\n",
    "        else:  # 多い場合は最初の3個だけ\n",
    "            for fid in file_ids[:3]:\n",
    "                print(f\"    - {fid}\")\n",
    "            print(f\"    ... 他{len(file_ids)-3}個\")\n",
    "        print()\n",
    "    \n",
    "    # 2. アノテーションデータの内容をチェック\n",
    "    print(\"=== アノテーションデータの内容チェック ===\")\n",
    "    sample_ids = list(json_data.get('files', {}).keys())[:3]\n",
    "    \n",
    "    for file_id in sample_ids:\n",
    "        data = json_data['files'][file_id]\n",
    "        print(f\"ファイルID: {file_id}\")\n",
    "        print(f\"  main_region数: {len(data.get('main_region', []))}\")\n",
    "        print(f\"  main_affinity数: {len(data.get('main_affinity', []))}\")\n",
    "        print(f\"  furi_region数: {len(data.get('furi_region', []))}\")\n",
    "        print(f\"  furi_affinity数: {len(data.get('furi_affinity', []))}\")\n",
    "        \n",
    "        # 座標データの妥当性チェック\n",
    "        if data.get('main_region'):\n",
    "            first_region = data['main_region'][0]\n",
    "            print(f\"  最初のregion座標: {first_region}\")\n",
    "            \n",
    "            # 座標が画像サイズ内に収まっているかチェック\n",
    "            # (通常は0-数千の範囲)\n",
    "            coords = [float(x) for x in first_region]\n",
    "            if any(c < 0 or c > 10000 for c in coords):\n",
    "                print(f\"  ⚠️ 異常な座標値が検出されました: {coords}\")\n",
    "        print()\n",
    "    \n",
    "    return doc_id_groups\n",
    "\n",
    "# 調査実行\n",
    "investigate_id_generation_process()\n",
    "doc_groups = check_concurrent_processing_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f5a691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 並行処理リスク分析 ===\n",
      "\n",
      "🔍 generate_dateset.pyの並行処理構造:\n",
      "\n",
      "    main関数:\n",
      "    ├── ProcessPoolExecutor(max_workers=20)  ← 20並列プロセス\n",
      "    │   ├── process1: main_exe_for_one_image()\n",
      "    │   ├── process2: main_exe_for_one_image()\n",
      "    │   ├── ...\n",
      "    │   └── process20: main_exe_for_one_image()\n",
      "    │\n",
      "    └── 各プロセスが同じJSONファイルに書き込み\n",
      "        └── update_json_data() ← ここで競合発生の可能性\n",
      "    \n",
      "⚠️ 特定された問題点:\n",
      "\n",
      "1. 【ファイルロック競合】\n",
      "   - 20個のプロセスが同時に同じJSONファイルにアクセス\n",
      "   - 'with lock:' はプロセス間ロックではなくスレッド間ロック\n",
      "   - multiprocessing.Manager().Lock() が必要\n",
      "\n",
      "2. 【JSONファイル全体読み書き】\n",
      "   - update_json_data()で毎回JSONファイル全体を読み込み\n",
      "   - json_data['files'][file_id] = data で上書き\n",
      "   - 同時書き込みでデータが失われる可能性\n",
      "\n",
      "3. 【レースコンディション】\n",
      "   プロセスA: JSONファイル読み込み → 処理中\n",
      "   プロセスB: 同じJSONファイル読み込み → 処理中\n",
      "   プロセスA: データ追加して保存\n",
      "   プロセスB: 古いデータに追加して保存 ← プロセスAの変更が消失\n",
      "\n",
      "4. 【ID重複の可能性】\n",
      "   - ファイルパス解析でのdoc_id取得\n",
      "   - 同じdoc_idの異なるファイルの処理タイミング\n",
      "   - 並行処理での上書き競合\n",
      "\n",
      "=== レースコンディション シミュレーション ===\n",
      "\n",
      "【正常な処理順序】\n",
      "時刻1: プロセスA - JSONファイル読み込み {}\n",
      "時刻2: プロセスA - image1のデータ処理\n",
      "時刻3: プロセスA - JSONに{'image1': data1}追加保存\n",
      "時刻4: プロセスB - JSONファイル読み込み {'image1': data1}\n",
      "時刻5: プロセスB - image2のデータ処理\n",
      "時刻6: プロセスB - JSONに{'image1': data1, 'image2': data2}保存\n",
      "\n",
      "【問題のある処理順序（レースコンディション）】\n",
      "時刻1: プロセスA - JSONファイル読み込み {}\n",
      "時刻2: プロセスB - JSONファイル読み込み {} (同じ空データ)\n",
      "時刻3: プロセスA - image1のデータ処理\n",
      "時刻4: プロセスB - image2のデータ処理\n",
      "時刻5: プロセスA - JSONに{'image1': data1}保存\n",
      "時刻6: プロセスB - JSONに{'image2': data2}保存 ← image1のデータが消失!\n",
      "時刻7: または、プロセスBがimage1用のデータをimage2に誤って結合\n",
      "\n",
      "【結果】\n",
      "- image1の画像に、image2のアノテーションが結び付く\n",
      "- または、image1のアノテーションが完全に失われる\n",
      "- JSONファイルにはimage2のデータのみ残る\n",
      "\n",
      "=== 解決策の提案 ===\n",
      "\n",
      "🔧 【緊急対策】\n",
      "1. 並行処理を無効化\n",
      "   - max_workers=1 に設定\n",
      "   - または ProcessPoolExecutor を使わず順次処理\n",
      "\n",
      "2. プロセス間ロックの実装\n",
      "\n",
      "    from multiprocessing import Manager\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        with Manager() as manager:\n",
      "            file_lock = manager.Lock()  # プロセス間ロック\n",
      "            with ProcessPoolExecutor(max_workers=20) as executor:\n",
      "                futures = []\n",
      "                for file_path in file_path_list:\n",
      "                    futures.append(executor.submit(\n",
      "                        main_exe_for_one_image,\n",
      "                        file_path=file_path,\n",
      "                        json_gt=json_gt,\n",
      "                        file_lock=file_lock  # ロックを渡す\n",
      "                    ))\n",
      "    \n",
      "3. 個別JSONファイル方式\n",
      "   - 各画像のアノテーションを個別ファイルに保存\n",
      "   - 最後に全てを統合\n",
      "   - 例: image1.json, image2.json → 最終的に統合\n",
      "\n",
      "🔧 【根本的解決策】\n",
      "1. データベース使用 (SQLite)\n",
      "2. キュー方式での順次書き込み\n",
      "3. 事前にファイルIDリストを作成し、重複チェック\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate_dateset.pyの並行処理問題を詳細調査\n",
    "def analyze_concurrent_processing_risks():\n",
    "    \"\"\"generate_dateset.pyの並行処理リスクを分析\"\"\"\n",
    "    \n",
    "    print(\"=== 並行処理リスク分析 ===\\n\")\n",
    "    \n",
    "    print(\"🔍 generate_dateset.pyの並行処理構造:\")\n",
    "    print(\"\"\"\n",
    "    main関数:\n",
    "    ├── ProcessPoolExecutor(max_workers=20)  ← 20並列プロセス\n",
    "    │   ├── process1: main_exe_for_one_image()\n",
    "    │   ├── process2: main_exe_for_one_image()\n",
    "    │   ├── ...\n",
    "    │   └── process20: main_exe_for_one_image()\n",
    "    │\n",
    "    └── 各プロセスが同じJSONファイルに書き込み\n",
    "        └── update_json_data() ← ここで競合発生の可能性\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"⚠️ 特定された問題点:\\n\")\n",
    "    \n",
    "    print(\"1. 【ファイルロック競合】\")\n",
    "    print(\"   - 20個のプロセスが同時に同じJSONファイルにアクセス\")\n",
    "    print(\"   - 'with lock:' はプロセス間ロックではなくスレッド間ロック\")\n",
    "    print(\"   - multiprocessing.Manager().Lock() が必要\\n\")\n",
    "    \n",
    "    print(\"2. 【JSONファイル全体読み書き】\")\n",
    "    print(\"   - update_json_data()で毎回JSONファイル全体を読み込み\")\n",
    "    print(\"   - json_data['files'][file_id] = data で上書き\")\n",
    "    print(\"   - 同時書き込みでデータが失われる可能性\\n\")\n",
    "    \n",
    "    print(\"3. 【レースコンディション】\")\n",
    "    print(\"   プロセスA: JSONファイル読み込み → 処理中\")\n",
    "    print(\"   プロセスB: 同じJSONファイル読み込み → 処理中\")\n",
    "    print(\"   プロセスA: データ追加して保存\")\n",
    "    print(\"   プロセスB: 古いデータに追加して保存 ← プロセスAの変更が消失\\n\")\n",
    "    \n",
    "    print(\"4. 【ID重複の可能性】\")\n",
    "    print(\"   - ファイルパス解析でのdoc_id取得\")\n",
    "    print(\"   - 同じdoc_idの異なるファイルの処理タイミング\")\n",
    "    print(\"   - 並行処理での上書き競合\\n\")\n",
    "\n",
    "def simulate_race_condition():\n",
    "    \"\"\"レースコンディションのシミュレーション\"\"\"\n",
    "    \n",
    "    print(\"=== レースコンディション シミュレーション ===\\n\")\n",
    "    \n",
    "    print(\"【正常な処理順序】\")\n",
    "    print(\"時刻1: プロセスA - JSONファイル読み込み {}\")\n",
    "    print(\"時刻2: プロセスA - image1のデータ処理\")\n",
    "    print(\"時刻3: プロセスA - JSONに{'image1': data1}追加保存\")\n",
    "    print(\"時刻4: プロセスB - JSONファイル読み込み {'image1': data1}\")\n",
    "    print(\"時刻5: プロセスB - image2のデータ処理\")\n",
    "    print(\"時刻6: プロセスB - JSONに{'image1': data1, 'image2': data2}保存\\n\")\n",
    "    \n",
    "    print(\"【問題のある処理順序（レースコンディション）】\")\n",
    "    print(\"時刻1: プロセスA - JSONファイル読み込み {}\")\n",
    "    print(\"時刻2: プロセスB - JSONファイル読み込み {} (同じ空データ)\")\n",
    "    print(\"時刻3: プロセスA - image1のデータ処理\")\n",
    "    print(\"時刻4: プロセスB - image2のデータ処理\")\n",
    "    print(\"時刻5: プロセスA - JSONに{'image1': data1}保存\")\n",
    "    print(\"時刻6: プロセスB - JSONに{'image2': data2}保存 ← image1のデータが消失!\")\n",
    "    print(\"時刻7: または、プロセスBがimage1用のデータをimage2に誤って結合\\n\")\n",
    "    \n",
    "    print(\"【結果】\")\n",
    "    print(\"- image1の画像に、image2のアノテーションが結び付く\")\n",
    "    print(\"- または、image1のアノテーションが完全に失われる\")\n",
    "    print(\"- JSONファイルにはimage2のデータのみ残る\\n\")\n",
    "\n",
    "def recommend_solutions():\n",
    "    \"\"\"解決策の提案\"\"\"\n",
    "    \n",
    "    print(\"=== 解決策の提案 ===\\n\")\n",
    "    \n",
    "    print(\"🔧 【緊急対策】\")\n",
    "    print(\"1. 並行処理を無効化\")\n",
    "    print(\"   - max_workers=1 に設定\")\n",
    "    print(\"   - または ProcessPoolExecutor を使わず順次処理\\n\")\n",
    "    \n",
    "    print(\"2. プロセス間ロックの実装\")\n",
    "    print(\"\"\"\n",
    "    from multiprocessing import Manager\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        with Manager() as manager:\n",
    "            file_lock = manager.Lock()  # プロセス間ロック\n",
    "            with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "                futures = []\n",
    "                for file_path in file_path_list:\n",
    "                    futures.append(executor.submit(\n",
    "                        main_exe_for_one_image,\n",
    "                        file_path=file_path,\n",
    "                        json_gt=json_gt,\n",
    "                        file_lock=file_lock  # ロックを渡す\n",
    "                    ))\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"3. 個別JSONファイル方式\")\n",
    "    print(\"   - 各画像のアノテーションを個別ファイルに保存\")\n",
    "    print(\"   - 最後に全てを統合\")\n",
    "    print(\"   - 例: image1.json, image2.json → 最終的に統合\\n\")\n",
    "    \n",
    "    print(\"🔧 【根本的解決策】\")\n",
    "    print(\"1. データベース使用 (SQLite)\")\n",
    "    print(\"2. キュー方式での順次書き込み\")\n",
    "    print(\"3. 事前にファイルIDリストを作成し、重複チェック\\n\")\n",
    "\n",
    "# 分析実行\n",
    "analyze_concurrent_processing_risks()\n",
    "simulate_race_condition()\n",
    "recommend_solutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4aa193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 緊急対策: generate_dateset.py の修正 ===\n",
      "\n",
      "🚨 現在のコード (問題あり):\n",
      "\n",
      "    if __name__ == \"__main__\":\n",
      "        with Manager() as manager:\n",
      "            with ProcessPoolExecutor(max_workers=20) as executor:  # ← 20並列\n",
      "                for doc_path in doc_path_list:\n",
      "                    for file_path in file_path_list:\n",
      "                        if procedure_for_one_image != None:\n",
      "                            futures.append(executor.submit(\n",
      "                                main_exe_for_one_image, \n",
      "                                file_path=file_path,\n",
      "                                json_gt=json_gt,\n",
      "                                procedure_for_one_image=procedure_for_one_image\n",
      "                            ))\n",
      "    \n",
      "✅ 修正版1: 並行処理無効化\n",
      "\n",
      "    if __name__ == \"__main__\":\n",
      "        with Manager() as manager:\n",
      "            with ProcessPoolExecutor(max_workers=1) as executor:  # ← 1に変更\n",
      "                for doc_path in doc_path_list:\n",
      "                    for file_path in file_path_list:\n",
      "                        if procedure_for_one_image != None:\n",
      "                            futures.append(executor.submit(\n",
      "                                main_exe_for_one_image, \n",
      "                                file_path=file_path,\n",
      "                                json_gt=json_gt,\n",
      "                                procedure_for_one_image=procedure_for_one_image\n",
      "                            ))\n",
      "    \n",
      "✅ 修正版2: 完全に順次処理\n",
      "\n",
      "    if __name__ == \"__main__\":\n",
      "        for doc_path in doc_path_list:\n",
      "            for file_path in file_path_list:\n",
      "                if procedure_for_one_image != None:\n",
      "                    # 直接実行（並行処理なし）\n",
      "                    main_exe_for_one_image(\n",
      "                        file_path=file_path,\n",
      "                        json_gt=json_gt,\n",
      "                        procedure_for_one_image=procedure_for_one_image\n",
      "                    )\n",
      "    \n",
      "⚠️ 注意点:\n",
      "- 処理時間は20倍長くなりますが、データの整合性が保たれます\n",
      "- 既存のJSONファイルはバックアップを取ってから再生成してください\n",
      "- 長期的にはプロセス間ロックまたは個別ファイル方式への移行を推奨\n",
      "\n",
      "🔧 バックアップ手順:\n",
      "1. 現在のgt_json.jsonをバックアップ\n",
      "2. generate_dateset.pyを修正版で実行\n",
      "3. 結果を確認して問題がないことを検証\n"
     ]
    }
   ],
   "source": [
    "def emergency_fix_suggestion():\n",
    "    \"\"\"緊急対策の具体的な修正提案\"\"\"\n",
    "    \n",
    "    print(\"=== 緊急対策: generate_dateset.py の修正 ===\\n\")\n",
    "    \n",
    "    print(\"🚨 現在のコード (問題あり):\")\n",
    "    print(\"\"\"\n",
    "    if __name__ == \"__main__\":\n",
    "        with Manager() as manager:\n",
    "            with ProcessPoolExecutor(max_workers=20) as executor:  # ← 20並列\n",
    "                for doc_path in doc_path_list:\n",
    "                    for file_path in file_path_list:\n",
    "                        if procedure_for_one_image != None:\n",
    "                            futures.append(executor.submit(\n",
    "                                main_exe_for_one_image, \n",
    "                                file_path=file_path,\n",
    "                                json_gt=json_gt,\n",
    "                                procedure_for_one_image=procedure_for_one_image\n",
    "                            ))\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"✅ 修正版1: 並行処理無効化\")\n",
    "    print(\"\"\"\n",
    "    if __name__ == \"__main__\":\n",
    "        with Manager() as manager:\n",
    "            with ProcessPoolExecutor(max_workers=1) as executor:  # ← 1に変更\n",
    "                for doc_path in doc_path_list:\n",
    "                    for file_path in file_path_list:\n",
    "                        if procedure_for_one_image != None:\n",
    "                            futures.append(executor.submit(\n",
    "                                main_exe_for_one_image, \n",
    "                                file_path=file_path,\n",
    "                                json_gt=json_gt,\n",
    "                                procedure_for_one_image=procedure_for_one_image\n",
    "                            ))\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"✅ 修正版2: 完全に順次処理\")\n",
    "    print(\"\"\"\n",
    "    if __name__ == \"__main__\":\n",
    "        for doc_path in doc_path_list:\n",
    "            for file_path in file_path_list:\n",
    "                if procedure_for_one_image != None:\n",
    "                    # 直接実行（並行処理なし）\n",
    "                    main_exe_for_one_image(\n",
    "                        file_path=file_path,\n",
    "                        json_gt=json_gt,\n",
    "                        procedure_for_one_image=procedure_for_one_image\n",
    "                    )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"⚠️ 注意点:\")\n",
    "    print(\"- 処理時間は20倍長くなりますが、データの整合性が保たれます\")\n",
    "    print(\"- 既存のJSONファイルはバックアップを取ってから再生成してください\")\n",
    "    print(\"- 長期的にはプロセス間ロックまたは個別ファイル方式への移行を推奨\\n\")\n",
    "    \n",
    "    print(\"🔧 バックアップ手順:\")\n",
    "    print(\"1. 現在のgt_json.jsonをバックアップ\")\n",
    "    print(\"2. generate_dateset.pyを修正版で実行\") \n",
    "    print(\"3. 結果を確認して問題がないことを検証\")\n",
    "    \n",
    "emergency_fix_suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517df6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
